# Only-Test 项目完整问答文档

## 📋 基于现有设计的确定答案

### Q1: 项目的核心问题是什么？
**A:** 控件名通过人工找到或录制太费时间，需要让外部LLM使用MCP监控屏幕获取当前控件状态（如 `com.mobile.brasiltvmobile:id/mImageFullScreen`），然后记录操作到JSON。

### Q2: 标准用例格式是什么？
**A:** `example_airtest_record.py` 是唯一的标准格式，是最终用例的执行格式。内部必须包含标准注释：`## [page] vod_playing_detail, [action] click, [comment] 点击全屏按钮进入全屏播放模式`。setup_hook等是辅助项，最终用例必须是airtest认可的简洁格式。

### Q3: LLM的具体作用范围？
**A:** 双重作用：
1. **元素识别和定位**：找到控件的resource_id等信息
2. **测试逻辑规划**：自驱动理解测试功能点，一步步捕获屏幕ID并写JSON用例

### Q4: JSON结构中的path字段作用？
**A:** 记录LLM使用工具生成的结果，用于追溯。包含：
- LLM使用了哪个MCP工具
- 截取了哪个屏幕  
- 分析了哪些元素
- 如何找到目标控件的
- 记录用例执行过程中相关信息

### Q5: 双模式识别机制？
**A:** 
- **uiautomator2模式**：速度快，准确率高，但在播放状态下无法获取界面控件布局
- **omniparser模式**：基于AI视觉识别，能识别播放状态下的控件，但速度慢，准确率90%，消耗GPU资源
- **自动切换**：根据播放状态自动选择合适的识别模式
- 对于外部LLM来说其可以不关注，因为当前拥有自动切换机制（播放状态下自动使用omniparser模式）
- **重要要求**：确保omniparser模式返回结果与uiautomator2模式相同

### Q6: MCP工具的具体作用？
**A:** 让外部LLM主动capture当前屏幕，并filter其需要的内容，获取id等控件信息后才能正常确认控件ID等信息，才能写用例。

## 🔧 技术实现详解

### Q7: 播放状态检测机制如何实现？
**A:** 使用ADB命令检测音频播放状态：
```bash
# 非播放状态
adb shell dumpsys media.audio_flinger | grep "Standby: no"  # 返回一条记录
adb shell dumpsys power | grep -i wake | grep Audio        # 仅AudioIn记录

# 播放状态  
adb shell dumpsys media.audio_flinger | grep "Standby: no"  # 返回两条记录
adb shell dumpsys power | grep -i wake | grep Audio        # 包含AudioOut记录
```
具体检测逻辑在 `airtest/lib/visual_recognition/playback_detector.py` 实现。

### Q8: 控件识别的fallback机制是什么？
**A:** 
**流程：**
1. 优先尝试uiautomator2获取控件
2. 如果失败或检测到播放状态，自动切换到omniparser
3. omniparser返回的结果需要转换为和uiautomator2一样的格式（resource_id、bbox等）

**处理策略：**
- **omniparser失败**：抛出警告，暂停生成流程，记录详细的警告信息
- **结果一致性**：通过代码兼容确保最终返回给外部LLM的json是相同的
- **特殊情况**：omniparser只做icon识别，不会返回id等信息，无法构造。当id等字段为空时（依靠 `airtest/templates/prompts/generate_cases.py` 进行prompt驱动外部LLM），直接使用bbox计算出的position进行点击具体坐标

### Q9: LLM工作流程是怎样的？
**A:** 
**标准流程：**
1. 分析当前需要执行的操作（如"进入全屏"）
2. 使用MCP工具capture当前屏幕
3. 自动选择识别模式（不需要LLM主动声明，MCP程序自动侦测状态并切换）
4. 分析识别结果，找到目标控件
5. 记录控件信息到JSON的path字段
6. 生成下一步的测试步骤

**智能决策：**
- **控件理解**：基于LLM对程序的理解能力，找到最像的那个id/name/text进行click
- **多控件选择**：选择概率最高的那个，在description中备注有多个可能，提示用户自己尝试
- **操作成功判断**：使用操作前后的截图进行相似度比较，如果相似度超过99%即为行为失败

### Q10: JSON到PY的转换规则？
**A:** 
**基本转换：**
- JSON中每个step对应一个标准注释 + 一行代码
- `target_element.resource_id` 转换为 `poco("resource_id").click()`
- `path` 字段不转换到PY，只用于调试和追溯
- `pre_action` 和 `post_action` 转换为额外的代码行

**特殊处理：**
- **复杂selector_path**：与多控件选择策略相同
- **坐标点击**：如果是omniparser模式，json中id等字段为空，使用 `poco.click([x,y])` 进行坐标点击
- **条件判断**：代码块使用统一的方式，转换时直接把特殊标志位的内容按顺序写入到py文件

## 🎯 设计理念与架构

### Q11: 为什么要设计这个框架？
**A:** 传统测试框架存在痛点：
- ❌ **硬编码坐标**: 换个设备就失效
- ❌ **静态逻辑**: 无法处理动态UI状态  
- ❌ **复杂编程**: 需要深厚技术背景
- ❌ **维护困难**: UI变更需要重写代码

Only-Test通过**JSON + Python协作架构**解决这些问题，实现"说出你的测试需求，剩下的交给AI"。

### Q12: 核心架构设计思想是什么？
**A:** 
```
自然语言 → LLM理解 → JSON智能元数据 → Python执行代码 → 测试报告
    ↓         ↓           ↓               ↓           ↓
   意图      逻辑        存储            执行        结果
```

**JSON作为智能媒介**：统计友好、AI友好、人类可读、版本控制友好
**Python作为执行载体**：灵活强大、生态丰富、调试友好、扩展性强

### Q13: 为什么需要设备密度适配？
**A:** 同样的UI元素在不同密度设备上大小差异巨大。解决方案包括：
- 🎯 **坐标智能缩放**: 根据密度比例自动调整触摸坐标
- 📸 **截图质量优化**: 高密度设备降低质量减少存储，低密度设备提高质量保证识别
- 🔍 **识别阈值调节**: 高密度图像质量好设置高阈值，低密度图像宽松要求
- 📱 **UI元素预测**: 预测不同设备上元素的实际像素大小

## 🔄 完整工作流程

### Q14: 完整的工作流程是什么？
**A:** 核心4步骤工作流：

**步骤1: 智能用例生成**
```bash
# 用户输入自然语言
python tools/case_generator.py --description "测试需求" --app com.mobile.brasiltvmobile
```

**步骤2: 设备信息探测与适配**
```bash
# 自动探测设备信息并更新JSON
python lib/device_adapter.py testcase.json
```

**步骤3: 智能执行与资源保存**
```bash
# 执行测试并保存完整资源
python tools/test_runner.py --file testcase.json
```

**步骤4: 数据回写与代码生成**
```bash
# JSON转换为Python代码
python lib/code_generator/json_to_python.py testcase.json
```

### Q15: 条件分支逻辑如何处理？
**A:** 用户描述"如果搜索框有内容先清空"自动转换为：
```json
{
  "action": "conditional_action",
  "condition": {
    "type": "element_content_check", 
    "target": "search_input_box",
    "check": "has_text_content"
  },
  "conditional_paths": {
    "if_has_content": {"action": "click", "target": "clear_button"},
    "if_empty": {"action": "input", "data": "搜索词"}
  }
}
```

### Q16: 资源路径管理规则是什么？
**A:** 
**命名规范**: `{pkg_name}_{device_name}`
- 示例：`com.mobile.brasiltvmobile` + `Pixel_6_Pro` = `com_mobile_brasiltvmobile_Pixel6Pro`
- 时间戳精确到毫秒: `step01_click_before_20241205_143022_123.png`
- 文件类型明确: `omni_result`, `element_screenshot`, `execution_log`

**存储结构**:
```
assets/
├── com_mobile_brasiltvmobile_Pixel6Pro/    # BrasilTVMobile+Pixel6Pro
├── com_mobile_brasiltvmobile_XiaomiPhone/  # BrasilTVMobile+小米手机
└── com_mobile_brasiltvmobile_HuaweiMate/   # BrasilTVMobile+华为Mate
```

## 🛠️ MCP工具完整清单

### Q17: MCP工具包含哪些功能？
**A:** 基于现有代码和设计，MCP工具包含：

**设备控制工具 (device)**
- **capture_screen**: 截取当前屏幕，LLM主动触发
- **analyze_ui_elements**: 分析界面元素，写json文件
- **detect_playing_state**: 检测播放状态，提供但不常用
- **click_element**: 点击指定元素
- **input_text**: 输入文本
- **swipe_screen**: 上下滑动操作，方便LLM操控设备
- **get_device_basic_info**: 获取设备基础信息
- **get_screen_info**: 获取屏幕信息
- **connect_device**: 连接目标设备

**生成工具 (generator)**  
- **generate_case_with_llm_guidance**: LLM指导的用例生成
- **convert_case_to_python**: JSON到Python转换
- **get_comprehensive_device_info**: 获取综合设备信息

**反馈工具 (feedback)**
- **execute_and_analyze**: 执行并分析测试
- **analyze_execution_result**: 分析执行结果

**工作流程工具 (workflow)**
- **start_complete_workflow**: 启动完整工作流程

## 📊 错误处理和重试机制

### Q18: 重试策略是什么？
**A:** 
- **控件找不到**：重置状态后重新进行到当前这一步后定位
- **omniparser识别错误**：可能fallback到预设坐标
- **操作失败**：LLM分析失败原因并调整策略

### Q19: 失败恢复机制是什么？
**A:** 
- **用例生成失败**：重置状态后重试一次
- **控件无法找到**：尝试重置状态（比如重新进入应用后尝试）
- **系统弹窗拦截**：外部LLM可能无法探查到下层控件信息时的处理

### Q20: 如何判断操作成功？
**A:** 使用操作前后的截图进行相似度比较，如果相似度超过99%即为行为失败。实现要求：找一个库进行相似度判断，一定要留下debug日志，如果行为失败抛出异常。

## 📁 测试数据管理

### Q21: 测试数据如何组织和存储？
**A:** 
**存储组织**
- **统一存储文件**：`testcases/main.yaml` 是设备和testsuits统一存储文件
- **资产路径规则**：`assets/{app}_{device}/` 按应用+设备分类存储
- **命名规范**：遵循 `airtest/README.md` 中定义的资源路径管理规则

### Q22: 状态管理和断点续传如何实现？
**A:** 
**断电续传任务设计：**
- 如果功能点输入被分为5个步骤，外部LLM操作，每完成一个都要记录一次状态
- 下次能查阅history并继续任务
- 状态文件：`workflow_state.json`, `execution_progress.json`, `iteration_config.json`
- 支持工作流程的暂停和恢复

### Q23: 配置回写机制是什么？
**A:** 
**动态配置更新：**
- `device_config.yaml`: 设备连接状态、基础信息、屏幕参数
- `framework_config.yaml`: 屏幕配置、识别参数
- `execution_config.json`: 执行环境配置
- `execution_stats.yaml`: 全局统计信息

**学习积累：**
- 应用UI模式学习数据库
- 元素选择器成功率统计
- 性能参数动态优化记录

## 🎯 测试用例可维护性

### Q24: 如何处理UI变化？
**A:** 
- **小变化**：LLM可以自动适应小的UI变化
- **重大改版**：定向重新生成用例，先不处理自动化更新
- **稳定性保证**：将用例文件转换为代码执行就是在增加稳定性

### Q25: 追溯支持如何实现？
**A:** 
- **path字段**：提供足够的信息用于调试，是一个统称，json中可能包含多个信息
- **执行轨迹**：完整记录每个操作步骤的执行过程
- **资产关联**：截图、分析结果、日志文件的完整关联

## 📁 目录结构说明

### Q26: 核心执行流程文件有哪些？
**A:** 
- **`example_airtest_record.py`** - 🔥 **最重要**：标准用例格式模板，所有生成的用例都要转换成这种格式
- **`json_to_python.py`** - JSON用例到Python执行脚本的转换器
- **`smart_executor.py`** - 智能执行引擎，处理复杂的执行逻辑

### Q27: LLM和MCP集成文件包括什么？
**A:** 
- **`mcp_server.py`** - 为外部LLM提供设备操作能力的MCP服务器
- **`llm_client.py`** - 与外部LLM通信的客户端
- **`workflow_orchestrator.py`** - 协调整个LLM驱动的测试生成流程

### Q28: 视觉识别核心文件是什么？
**A:** 
- **`omniparser_client.py`** - 与Omniparser服务器通信，处理播放状态下的视觉识别
- **`pure_uiautomator2_extractor.py`** - 提取uiautomator2的UI层次结构
- **`strategy_manager.py`** - 管理uiautomator2和Omniparser的智能切换
- **`playback_detector.py`** - 检测播放状态，决定使用哪种识别方式

## 📋 当前架构状态

### Q29: 已实现功能有哪些？
**A:** 基于现有代码：
- ✅ **MCP服务器框架** (`mcp_server.py`)
- ✅ **LLM客户端通信** (`llm_client.py`)  
- ✅ **Omniparser客户端** (`omniparser_client.py`)
- ✅ **设备适配器** (`device_adapter.py`)
- ✅ **JSON到Python转换器** (`json_to_python.py`)
- ✅ **提示词模板系统** (`generate_cases.py`)
- ✅ **工作流程编排** (`workflow_orchestrator.py`)
- ✅ **配置管理系统** (`config_manager.py`)
- ✅ **标准用例格式** (`example_airtest_record.py`)

### Q30: 核心工作流程是什么？
**A:** 
```
外部LLM → MCP工具 → 播放状态检测 → 策略选择 → 
(uiautomator2 或 omniparser) → 生成JSON用例 → 转换为标准Python执行脚本
```

### Q31: 测试用例的自动化程度如何？
**A:** 
**用户输入要求：**
- 提供具体的功能点描述，足够清晰，不是大而泛的描述
- 示例：\"测试vod点播播放正常: 1.进入APK后就是首页，执行关闭广告函数，2.找到searchbtn点击，直到可输入状态后输入节目名称'720'点击第一个节目，3.播放节目，断言: 验证设备是否处于播放状态\"

**自动化能力：**
- LLM自动规划测试路径（启动应用→搜索→播放→全屏）  
- LLM自动找到每个步骤需要的控件
- 最终生成完整可执行的测试用例

**异常处理：** 异常情况目前不考虑，直接设计为抛出并记录异常情况即可

### Q32: 性能和准确性如何平衡？
**A:** 
**策略：**
- 优先使用快速的uiautomator2
- 只在播放状态时使用omniparser  
- omniparser的90%准确率的10%错误无需理会，后期会确保准确率
- 暂不考虑缓存机制和性能监控

---

## 📝 最终总结

**Only-Test项目基于现有设计已经具备了完整的架构框架，核心理念是：**

1. **让外部LLM通过MCP工具实时感知设备状态**
2. **智能选择最合适的UI识别策略**  
3. **生成符合airtest标准格式的可执行测试用例**
4. **提供完整的执行追溯和状态管理能力**

**下一步重点是完善播放状态检测、策略切换逻辑和MCP工具的具体实现。**

---

*基于airtest目录现有代码和设计文档整理*  
*最后更新: 2025-09-09*  
*状态: 基于现有架构的确定答案*
