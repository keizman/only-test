#!/usr/bin/env python3
"""
Only-Test LLMæœåŠ¡å®¢æˆ·ç«¯

æ”¯æŒå¤šç§LLMæœåŠ¡æä¾›å•†çš„ç»Ÿä¸€æ¥å£
åŒ…æ‹¬OpenAIã€Claudeã€OpenAIå…¼å®¹æœåŠ¡ç­‰
æä¾›è‡ªåŠ¨é‡è¯•ã€é”™è¯¯å¤„ç†ã€å›é€€æœºåˆ¶
"""

import os
import json
import time
import logging
from typing import Dict, Any, List, Optional, Union
from dataclasses import dataclass
from abc import ABC, abstractmethod
from pathlib import Path
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent.parent))

from lib.config_manager import ConfigManager

# è®¾ç½®æ—¥å¿—
logger = logging.getLogger(__name__)


@dataclass
class LLMResponse:
    """LLMå“åº”æ•°æ®ç±»"""
    content: str
    model: str
    provider: str
    usage: Dict[str, Any]
    success: bool
    error: Optional[str] = None
    response_time: float = 0.0


@dataclass
class LLMMessage:
    """LLMæ¶ˆæ¯æ•°æ®ç±»"""
    role: str  # system, user, assistant
    content: str
    
    def to_dict(self) -> Dict[str, str]:
        return {"role": self.role, "content": self.content}


class BaseLLMProvider(ABC):
    """LLMæä¾›å•†åŸºç±»"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.timeout = config.get('timeout', 30)
        self.max_retries = config.get('max_retries', 3)
    
    @abstractmethod
    def chat_completion(self, messages: List[LLMMessage], **kwargs) -> LLMResponse:
        """èŠå¤©å®Œæˆæ¥å£"""
        pass
    
    @abstractmethod
    def is_available(self) -> bool:
        """æ£€æŸ¥æœåŠ¡æ˜¯å¦å¯ç”¨"""
        pass


class OpenAICompatibleProvider(BaseLLMProvider):
    """OpenAIå…¼å®¹æœåŠ¡æä¾›å•†"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        
        # å°è¯•å¯¼å…¥openaiåº“
        try:
            import openai
            self.openai = openai
            
            # é…ç½®å®¢æˆ·ç«¯
            self.client = openai.OpenAI(
                api_key=config.get('api_key'),
                base_url=config.get('api_url', 'https://api.openai.com/v1'),
                timeout=self.timeout
            )
            # Debug: åŸºæœ¬è¿æ¥ä¿¡æ¯ï¼ˆä¸æ‰“å°å¯†é’¥ï¼‰
            try:
                logger.info(f"LLM(OpenAICompatible) base_url={config.get('api_url')}, model={config.get('model')}")
            except Exception:
                pass
            
        except ImportError:
            logger.error("æœªå®‰è£…openaiåº“: pip install openai")
            self.client = None
    
    def chat_completion(self, messages: List[LLMMessage], **kwargs) -> LLMResponse:
        """OpenAIé£æ ¼çš„èŠå¤©å®Œæˆ"""
        if not self.client:
            return LLMResponse(
                content="",
                model=self.config.get('model', 'unknown'),
                provider="openai_compatible",
                usage={},
                success=False,
                error="OpenAIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–"
            )
        
        start_time = time.time()
        
        # å‡†å¤‡è¯·æ±‚å‚æ•°
        request_params = {
            "model": self.config.get('model', 'gpt-3.5-turbo'),
            "messages": [msg.to_dict() for msg in messages],
            "temperature": kwargs.get('temperature', self.config.get('temperature', 0.7)),
            "max_tokens": kwargs.get('max_tokens', self.config.get('max_tokens', 2000))
        }
        
        # æ·»åŠ å…¶ä»–å‚æ•°
        if 'top_p' in kwargs:
            request_params['top_p'] = kwargs['top_p']
        if 'frequency_penalty' in kwargs:
            request_params['frequency_penalty'] = kwargs['frequency_penalty']
        
        for attempt in range(self.max_retries):
            try:
                response = self.client.chat.completions.create(**request_params)
                
                response_time = time.time() - start_time
                
                return LLMResponse(
                    content=response.choices[0].message.content,
                    model=response.model,
                    provider="openai_compatible",
                    usage={
                        "prompt_tokens": response.usage.prompt_tokens,
                        "completion_tokens": response.usage.completion_tokens,
                        "total_tokens": response.usage.total_tokens
                    },
                    success=True,
                    response_time=response_time
                )
                
            except Exception as e:
                logger.warning(f"LLMè¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{self.max_retries}): {e}")
                if attempt < self.max_retries - 1:
                    time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                else:
                    return LLMResponse(
                        content="",
                        model=self.config.get('model', 'unknown'),
                        provider="openai_compatible",
                        usage={},
                        success=False,
                        error=str(e),
                        response_time=time.time() - start_time
                    )
    
    def is_available(self) -> bool:
        """æ£€æŸ¥æœåŠ¡å¯ç”¨æ€§"""
        if not self.client:
            return False
        
        try:
            # å‘é€ç®€å•æµ‹è¯•è¯·æ±‚
            test_messages = [LLMMessage("user", "Hello")]
            response = self.chat_completion(test_messages, max_tokens=1)
            return response.success
        except:
            return False


class OpenAIProvider(OpenAICompatibleProvider):
    """å®˜æ–¹OpenAIæœåŠ¡æä¾›å•†"""
    
    def __init__(self, config: Dict[str, Any]):
        # è®¾ç½®å®˜æ–¹API URL
        config['api_url'] = 'https://api.openai.com/v1'
        super().__init__(config)


class ClaudeProvider(BaseLLMProvider):
    """Anthropic ClaudeæœåŠ¡æä¾›å•†"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        
        try:
            import anthropic
            self.client = anthropic.Anthropic(
                api_key=config.get('api_key'),
                timeout=self.timeout
            )
        except ImportError:
            logger.error("æœªå®‰è£…anthropicåº“: pip install anthropic")
            self.client = None
    
    def chat_completion(self, messages: List[LLMMessage], **kwargs) -> LLMResponse:
        """Claudeé£æ ¼çš„æ¶ˆæ¯å®Œæˆ"""
        if not self.client:
            return LLMResponse(
                content="",
                model=self.config.get('model', 'claude-3'),
                provider="claude",
                usage={},
                success=False,
                error="Claudeå®¢æˆ·ç«¯æœªåˆå§‹åŒ–"
            )
        
        start_time = time.time()
        
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼ (Claudeè¦æ±‚ä¸åŒçš„æ ¼å¼)
        claude_messages = []
        system_message = ""
        
        for msg in messages:
            if msg.role == "system":
                system_message = msg.content
            else:
                claude_messages.append(msg.to_dict())
        
        for attempt in range(self.max_retries):
            try:
                response = self.client.messages.create(
                    model=self.config.get('model', 'claude-3-sonnet-20240229'),
                    max_tokens=kwargs.get('max_tokens', self.config.get('max_tokens', 2000)),
                    temperature=kwargs.get('temperature', self.config.get('temperature', 0.7)),
                    system=system_message,
                    messages=claude_messages
                )
                
                response_time = time.time() - start_time
                
                return LLMResponse(
                    content=response.content[0].text,
                    model=response.model,
                    provider="claude",
                    usage={
                        "input_tokens": response.usage.input_tokens,
                        "output_tokens": response.usage.output_tokens,
                        "total_tokens": response.usage.input_tokens + response.usage.output_tokens
                    },
                    success=True,
                    response_time=response_time
                )
                
            except Exception as e:
                logger.warning(f"Claudeè¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{self.max_retries}): {e}")
                if attempt < self.max_retries - 1:
                    time.sleep(2 ** attempt)
                else:
                    return LLMResponse(
                        content="",
                        model=self.config.get('model', 'claude-3'),
                        provider="claude",
                        usage={},
                        success=False,
                        error=str(e),
                        response_time=time.time() - start_time
                    )
    
    def is_available(self) -> bool:
        """æ£€æŸ¥ClaudeæœåŠ¡å¯ç”¨æ€§"""
        if not self.client:
            return False
        
        try:
            test_messages = [LLMMessage("user", "Hi")]
            response = self.chat_completion(test_messages, max_tokens=1)
            return response.success
        except:
            return False


class LLMClient:
    """ç»Ÿä¸€LLMå®¢æˆ·ç«¯"""
    
    def __init__(self, config_manager: Optional[ConfigManager] = None):
        """
        åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        
        Args:
            config_manager: é…ç½®ç®¡ç†å™¨å®ä¾‹
        """
        self.config_manager = config_manager or ConfigManager()
        self.llm_config = self.config_manager.get_llm_config()
        
        # åˆå§‹åŒ–æä¾›å•†
        self.providers = {}
        self._init_providers()
        
        # å½“å‰æ´»è·ƒæä¾›å•†
        self.active_provider = None
        self._select_active_provider()
    
    def _init_providers(self):
        """åˆå§‹åŒ–LLMæä¾›å•†"""
        provider_type = self.llm_config.get('provider', 'openai_compatible')
        
        # ä¸»æä¾›å•†é…ç½®
        main_config = {
            'api_key': os.getenv('LLM_API_KEY'),
            'api_url': os.getenv('LLM_API_URL'),
            'model': os.getenv('LLM_MODEL'),
            'temperature': float(os.getenv('LLM_TEMPERATURE', '0.7')),
            'max_tokens': int(os.getenv('LLM_MAX_TOKENS', '2000')),
            'timeout': int(os.getenv('LLM_TIMEOUT', '30'))
        }
        
        # æ ¹æ®ç±»å‹åˆå§‹åŒ–ä¸»æä¾›å•†
        if provider_type == 'openai_compatible':
            self.providers['main'] = OpenAICompatibleProvider(main_config)
        elif provider_type == 'openai':
            self.providers['main'] = OpenAIProvider(main_config)
        elif provider_type == 'claude':
            self.providers['main'] = ClaudeProvider(main_config)
        else:
            logger.error(f"ä¸æ”¯æŒçš„LLMæä¾›å•†ç±»å‹: {provider_type}")
        
        # å›é€€æä¾›å•†é…ç½®
        fallback_provider = os.getenv('LLM_FALLBACK_PROVIDER')
        fallback_api_key = os.getenv('LLM_FALLBACK_API_KEY')
        
        if fallback_provider and fallback_api_key:
            fallback_config = {
                'api_key': fallback_api_key,
                'model': os.getenv('LLM_FALLBACK_MODEL', 'gpt-3.5-turbo'),
                'temperature': 0.7,
                'max_tokens': 2000,
                'timeout': 30
            }
            
            if fallback_provider == 'openai':
                self.providers['fallback'] = OpenAIProvider(fallback_config)
            elif fallback_provider == 'claude':
                self.providers['fallback'] = ClaudeProvider(fallback_config)
    
    def _select_active_provider(self):
        """é€‰æ‹©æ´»è·ƒçš„æä¾›å•†"""
        # ä¼˜å…ˆä½¿ç”¨ä¸»æä¾›å•†
        if 'main' in self.providers and self.providers['main'].is_available():
            self.active_provider = self.providers['main']
            logger.info("ä½¿ç”¨ä¸»LLMæä¾›å•†")
            return
        
        # å›é€€åˆ°å¤‡ç”¨æä¾›å•†
        if 'fallback' in self.providers and self.providers['fallback'].is_available():
            self.active_provider = self.providers['fallback']
            logger.warning("ä¸»LLMæä¾›å•†ä¸å¯ç”¨ï¼Œåˆ‡æ¢åˆ°å¤‡ç”¨æä¾›å•†")
            return
        
        logger.error("æ‰€æœ‰LLMæä¾›å•†éƒ½ä¸å¯ç”¨")
        self.active_provider = None
    
    def is_available(self) -> bool:
        """æ£€æŸ¥LLMæœåŠ¡æ˜¯å¦å¯ç”¨"""
        if not self.llm_config.get('enabled', True):
            return False
        
        return self.active_provider is not None
    
    def chat_completion(self, messages: Union[List[LLMMessage], List[Dict[str, str]]], **kwargs) -> LLMResponse:
        """
        èŠå¤©å®Œæˆæ¥å£
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            LLMResponse: å“åº”ç»“æœ
        """
        if not self.is_available():
            return LLMResponse(
                content="",
                model="unavailable",
                provider="unavailable",
                usage={},
                success=False,
                error="LLMæœåŠ¡ä¸å¯ç”¨"
            )
        
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        if messages and isinstance(messages[0], dict):
            messages = [LLMMessage(msg['role'], msg['content']) for msg in messages]
        
        # å°è¯•ä¸»æä¾›å•†
        response = self.active_provider.chat_completion(messages, **kwargs)
        
        # å¦‚æœä¸»æä¾›å•†å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æä¾›å•†
        if not response.success and 'fallback' in self.providers:
            logger.warning("ä¸»LLMæä¾›å•†å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æä¾›å•†")
            fallback_provider = self.providers['fallback']
            if fallback_provider.is_available():
                response = fallback_provider.chat_completion(messages, **kwargs)
        
        return response
    
    def generate_test_case(self, description: str, app_package: str, device_type: str = "android_phone") -> Optional[Dict[str, Any]]:
        """
        ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
        
        Args:
            description: æµ‹è¯•éœ€æ±‚æè¿°
            app_package: åº”ç”¨åŒ…å
            device_type: è®¾å¤‡ç±»å‹
            
        Returns:
            Dict: ç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹JSON
        """
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç§»åŠ¨ç«¯UIè‡ªåŠ¨åŒ–æµ‹è¯•ä¸“å®¶ï¼Œè´Ÿè´£å°†ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æè¿°è½¬æ¢ä¸ºç»“æ„åŒ–çš„æµ‹è¯•ç”¨ä¾‹JSONã€‚

è¯·æ ¹æ®ç”¨æˆ·æè¿°ç”Ÿæˆå®Œæ•´çš„æµ‹è¯•ç”¨ä¾‹ï¼ŒåŒ…å«ï¼š
1. æ¡ä»¶åˆ†æ”¯é€»è¾‘ (å¦‚æœ...å°±...)
2. æ™ºèƒ½å…ƒæ•°æ® (AIæç¤ºã€ä¸šåŠ¡é€»è¾‘è¯´æ˜)
3. å¤šé‡é€‰æ‹©å™¨ç­–ç•¥
4. æ–­è¨€éªŒè¯

è¿”å›æ ‡å‡†çš„Only-Test JSONæ ¼å¼ã€‚"""
        
        user_prompt = f"""
è¯·ä¸ºä»¥ä¸‹æµ‹è¯•éœ€æ±‚ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹:

åº”ç”¨åŒ…å: {app_package}
è®¾å¤‡ç±»å‹: {device_type}
æµ‹è¯•éœ€æ±‚: {description}

è¯·ç”ŸæˆåŒ…å«æ™ºèƒ½æ¡ä»¶é€»è¾‘çš„å®Œæ•´JSONæµ‹è¯•ç”¨ä¾‹ã€‚
"""
        
        messages = [
            LLMMessage("system", system_prompt),
            LLMMessage("user", user_prompt)
        ]
        
        response = self.chat_completion(messages, temperature=0.3)  # è¾ƒä½æ¸©åº¦ä¿è¯ç»“æ„åŒ–è¾“å‡º
        
        if not response.success:
            logger.error(f"LLMæµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå¤±è´¥: {response.error}")
            return None
        
        try:
            # å°è¯•è§£æJSON
            content = response.content.strip()
            if content.startswith('```json'):
                content = content[7:]
            if content.endswith('```'):
                content = content[:-3]
            
            return json.loads(content)
        except json.JSONDecodeError as e:
            logger.error(f"LLMè¿”å›å†…å®¹ä¸æ˜¯æœ‰æ•ˆJSON: {e}")
            logger.debug(f"åŸå§‹å†…å®¹: {response.content}")
            return None
    
    def review_test_case(self, test_case: Dict[str, Any]) -> Dict[str, Any]:
        """
        å®¡æŸ¥æµ‹è¯•ç”¨ä¾‹
        
        Args:
            test_case: æµ‹è¯•ç”¨ä¾‹JSON
            
        Returns:
            Dict: å®¡æŸ¥ç»“æœå’Œå»ºè®®
        """
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªæµ‹è¯•ç”¨ä¾‹å®¡æŸ¥ä¸“å®¶ï¼Œè´Ÿè´£å®¡æŸ¥ç§»åŠ¨ç«¯UIè‡ªåŠ¨åŒ–æµ‹è¯•ç”¨ä¾‹çš„è´¨é‡ã€‚

è¯·ä»ä»¥ä¸‹æ–¹é¢è¯„ä¼°æµ‹è¯•ç”¨ä¾‹ï¼š
1. é€»è¾‘å®Œæ•´æ€§
2. é€‰æ‹©å™¨å‡†ç¡®æ€§  
3. å¼‚å¸¸å¤„ç†
4. æ‰§è¡Œæ•ˆç‡
5. ç»´æŠ¤æ€§

è¿”å›JSONæ ¼å¼çš„å®¡æŸ¥æŠ¥å‘Šã€‚"""
        
        user_prompt = f"""
è¯·å®¡æŸ¥ä»¥ä¸‹æµ‹è¯•ç”¨ä¾‹:

{json.dumps(test_case, ensure_ascii=False, indent=2)}

è¿”å›å®¡æŸ¥æŠ¥å‘Šï¼ŒåŒ…å«è¯„åˆ†ã€é—®é¢˜åˆ—è¡¨å’Œæ”¹è¿›å»ºè®®ã€‚
"""
        
        messages = [
            LLMMessage("system", system_prompt),
            LLMMessage("user", user_prompt)
        ]
        
        response = self.chat_completion(messages, temperature=0.2)
        
        if not response.success:
            return {
                "success": False,
                "error": response.error,
                "score": 0,
                "issues": ["LLMå®¡æŸ¥æœåŠ¡ä¸å¯ç”¨"],
                "suggestions": []
            }
        
        try:
            content = response.content.strip()
            if content.startswith('```json'):
                content = content[7:-3]
            
            review_result = json.loads(content)
            review_result["success"] = True
            return review_result
        except json.JSONDecodeError:
            return {
                "success": False,
                "error": "å®¡æŸ¥ç»“æœè§£æå¤±è´¥",
                "raw_response": response.content
            }
    
    def get_usage_stats(self) -> Dict[str, Any]:
        """è·å–ä½¿ç”¨ç»Ÿè®¡"""
        return {
            "providers_available": len(self.providers),
            "active_provider": self.active_provider.__class__.__name__ if self.active_provider else None,
            "llm_enabled": self.llm_config.get('enabled', False),
            "model_preferences": self.llm_config.get('model_preferences', {})
        }


def main():
    """æµ‹è¯•LLMå®¢æˆ·ç«¯"""
    print("ğŸ¤– Only-Test LLMå®¢æˆ·ç«¯æµ‹è¯•")
    print("=" * 50)
    
    # åˆå§‹åŒ–å®¢æˆ·ç«¯
    try:
        llm_client = LLMClient()
        
        # æ£€æŸ¥å¯ç”¨æ€§
        if not llm_client.is_available():
            print("âŒ LLMæœåŠ¡ä¸å¯ç”¨")
            return
        
        print("âœ… LLMæœåŠ¡å¯ç”¨")
        
        # è·å–ä½¿ç”¨ç»Ÿè®¡
        stats = llm_client.get_usage_stats()
        print(f"ğŸ“Š ä½¿ç”¨ç»Ÿè®¡: {stats}")
        
        # æµ‹è¯•ç®€å•èŠå¤©
        test_messages = [
            LLMMessage("user", "è¯·ç®€å•ä»‹ç»ä¸€ä¸‹ç§»åŠ¨ç«¯UIè‡ªåŠ¨åŒ–æµ‹è¯•")
        ]
        
        print("\nğŸ”„ æµ‹è¯•èŠå¤©å®Œæˆ...")
        response = llm_client.chat_completion(test_messages, max_tokens=100)
        
        if response.success:
            print(f"âœ… å“åº”æˆåŠŸ")
            print(f"ğŸ“ å†…å®¹: {response.content[:200]}...")
            print(f"â±ï¸  è€—æ—¶: {response.response_time:.2f}ç§’")
            print(f"ğŸ”¢ Tokenä½¿ç”¨: {response.usage}")
        else:
            print(f"âŒ å“åº”å¤±è´¥: {response.error}")
        
        # æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆ
        print("\nğŸ§ª æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆ...")
        test_case = llm_client.generate_test_case(
            "åœ¨æŠ–éŸ³ä¸­æœç´¢ç¾é£Ÿè§†é¢‘ï¼Œå¦‚æœæœç´¢æ¡†æœ‰å†…å®¹å…ˆæ¸…ç©º",
            "com.mobile.brasiltvmobile"
        )
        
        if test_case:
            print("âœ… æµ‹è¯•ç”¨ä¾‹ç”ŸæˆæˆåŠŸ")
            print(f"ğŸ“‹ ç”¨ä¾‹åç§°: {test_case.get('name', 'Unknown')}")
            print(f"ğŸ”„ æ‰§è¡Œæ­¥éª¤: {len(test_case.get('execution_path', []))}")
        else:
            print("âŒ æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå¤±è´¥")
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
