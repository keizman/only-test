#!/usr/bin/env python3
"""
MCP + LLM Workflow Demo - Unified Logging & Auto-Refresh Strategy
===================================================================

æ ¸å¿ƒåŠŸèƒ½æ¨¡å—:
1. ç»Ÿä¸€æ—¥å¿—ç³»ç»Ÿ
   - session_unified.json (tag/phase/seq)
   - execution_log.json (JSONL, æ¯è¡Œä¸€æ¡è®°å½•)
   - æ‰€æœ‰prompt/response/tool/erroråˆ†ç¦»å­˜å‚¨åœ¨ prompts/ responses/ tools/ errors/ ç›®å½•

2. å±å¹•ä¿¡æ¯è¿‡æ»¤
   - ä»…å±•ç¤º target_app çš„å…ƒç´  (resource_id å‰ç¼€æˆ– package åŒ¹é…)
   - ä»…å±•ç¤ºæœ‰æ•ˆ selector (resource_id æˆ– text éç©º)
   - ç”Ÿæˆ selector æ¸…å• (resource_id/text å»é‡)

3. ä¸Šä¸‹æ–‡ç»´æŠ¤
   - last-N responses: æ¯è½®åŒ…å«ä¸ŠNè½®LLMè¾“å‡º
   - next_round_hint: LLMç»™å‡ºä¸‹ä¸€è½®æŒ‡å¯¼ï¼ˆæ–°å­—æ®µåï¼Œé¿å…ä¸current_actionæ··æ·†ï¼‰
   - invalid_note: ä¸Šä¸€è½®è‹¥å¤±è´¥åˆ™è®°å½•å¤±è´¥åŸå› 

4. è‡ªåŠ¨åˆ·æ–°ç­–ç•¥ (å…³é”®è®¾è®¡)
   - LLM è‹¥è¿”å› tool_request â†’ è‡ªåŠ¨æ‰§è¡Œ refresh
   - Refresh ååœ¨åŒä¸€è½®è¿›è¡ŒäºŒæ¬¡æé—® (å¸¦æ–°selector)
   - ç›®æ ‡: é¿å…é¦–å±å¡ä½ (å¹¿å‘Š/loading) æµªè´¹ä¸€è½®
   - çº¦æŸ: æœ€å¤šåˆ·æ–°1æ¬¡; äºŒæ¬¡æé—®åè‹¥ä»è¿”å›tool_requeståˆ™è®°å½•å¹¶ç»§ç»­ä¸‹ä¸€è½®
   - æ‰€æœ‰refreshè®°å½•åœ¨execution_logä¸­ (status: tool_request_executed)

5. æ—¥å¿—å…³é”®å­—æ®µè¯´æ˜
   - phase: session|plan|execution
   - round: æ‰§è¡Œè½®æ•° (1~max_rounds)
   - status: started|parsed|completed|tool_request_executed|failed
   - step_json: LLM è§£æåçš„å®Œæ•´è¾“å‡º (current_action æˆ– tool_request)
   - refresh_used: æ˜¯å¦åœ¨æœ¬è½®æ‰§è¡Œäº†å±å¹•åˆ·æ–°

è¿è¡Œç¤ºä¾‹:
  # ä»…è§„åˆ’å’Œè®°å½•ï¼ˆé»˜è®¤ï¼Œä¸æ‰§è¡Œ UI åŠ¨ä½œï¼‰:
  python -m only_test.workflows.mcp_llm_workflow_demo \
    --requirement "æ’­æ”¾VODèŠ‚ç›®" \
    --target-app com.mobile.brasiltvmobile \
    --max-rounds 10 \
    --verbose

  # çœŸå®æ‰§è¡Œ UI åŠ¨ä½œï¼ˆstepwiseï¼Œæ·»åŠ  --run-stepwiseï¼‰:
  python -m only_test.workflows.mcp_llm_workflow_demo \
    --requirement "æ’­æ”¾VODèŠ‚ç›®" \
    --target-app com.mobile.brasiltvmobile \
    --max-rounds 10 \
    --run-stepwise  # <-- å¯ç”¨æŒ‰å›åˆå®é™…æ‰§è¡Œï¼ˆMCPçº§åˆ«ï¼‰

  # ç”Ÿæˆå®Œæ•´ JSON â†’ è½¬ PY â†’ ç›´æ¥æ‰§è¡Œç”Ÿæˆçš„ Pythonï¼ˆæ·»åŠ  --executeï¼‰:
  python -m only_test.workflows.mcp_llm_workflow_demo \
    --requirement "æ’­æ”¾VODèŠ‚ç›®" \
    --target-app com.mobile.brasiltvmobile \
    --max-rounds 10 \
    --execute  # <-- ç”Ÿæˆå®Œæˆåè½¬æ¢ä¸º Python å¹¶å°è¯•ç›´æ¥æ‰§è¡Œ

å¯é€‰å‚æ•°:
  --run-stepwise          å¯ç”¨æŒ‰å›åˆ UI åŠ¨ä½œæ‰§è¡Œï¼ˆMCP å•æ­¥ï¼‰
  --execute               åœ¨å®Œæˆé˜¶æ®µï¼šå°†æœ€ç»ˆ JSON è½¬æ¢ä¸º Pythonï¼Œå¹¶å°è¯•ç›´æ¥æ‰§è¡Œ
  --max-rounds N          æœ€å¤§è½®æ•° (CLI > plan > é»˜è®¤10, ç»å¯¹ä¸Šé™20)
  --history-window N      contextçª—å£å¤§å° (é»˜è®¤ä»configè¯»å–æˆ–10)
  --verbose / -v          DEBUGçº§åˆ«æ§åˆ¶å°è¾“å‡º
  --auto-close-limit N    å¹¿å‘Šè‡ªåŠ¨å…³é—­å°è¯•æ•°

è°ƒæ•´ç­–ç•¥(å½“é¦–è½®å¡ä½æ—¶):
  å…³é—­è‡ªåŠ¨åˆ·æ–°: æ³¨é‡Šæˆ–åˆ é™¤ "# Re-prompt once with refreshed screen info" ä»£ç å—
  æ”¹promptå¼ºåº¦: è°ƒæ•´ "ç”Ÿæˆè§„åˆ™" ä¸­çš„æªè¾ï¼Œå¦‚æ”¹ä¸ºä»… wait/swipe
  æŸ¥çœ‹selector: æ£€æŸ¥ tool_get_current_screen_info_round_1.json ä¸­ elements å†…å®¹
  
  check `mcp_execution_log.json` and the discrete files under `tools/` (e.g., tool_*.json).
- Entries in `mcp_execution_log.json` only reflect MCP tool invocations and their results;
  non-tool events (planning, parsing, LLM text responses) are intentionally not recorded there.

Run:
  python -m only_test.workflows.mcp_llm_workflow_demo --requirement "..." --target-app com.xxx
"""

import argparse
import asyncio
import json
import logging
import os
import sys
from datetime import datetime
from pathlib import Path

# Ensure repository root on sys.path
_repo_root = str(Path(__file__).resolve().parents[2])
if _repo_root not in sys.path:
    sys.path.insert(0, _repo_root)

from only_test.lib.logging import get_logger
from only_test.lib.mcp_interface.mcp_server import MCPServer, MCPTool
from only_test.lib.llm_integration.llm_client import LLMClient


async def main():
    parser = argparse.ArgumentParser(description="Workflows MCP+LLM demo with improved prompts and logging")
    parser.add_argument("--requirement", required=True)
    parser.add_argument("--target-app", default="com.mobile.brasiltvmobile")
    parser.add_argument("--outdir", default="only_test/testcases/generated")
    parser.add_argument("--device-id", default=None)
    parser.add_argument("--logdir", default="logs/mcp_demo")
    parser.add_argument("--max-rounds", type=int, default=6)
    parser.add_argument("--auto-close-limit", type=int, default=None)
    parser.add_argument("--history-window", type=int, default=None, help="How many previous step responses to include (defaults from YAML or 10)")
    parser.add_argument("--run-stepwise", action="store_true", help="Execute current_action per round via MCP tools")
    parser.add_argument("--execute", action="store_true", help="After completion: convert final JSON to Python and try to execute")
    parser.add_argument("--verbose", "-v", action="store_true")
    args = parser.parse_args()

    session_id = datetime.now().strftime('%Y%m%d_%H%M%S')
    console_level = logging.DEBUG if args.verbose else logging.INFO
    logger = get_logger(session_id, args.logdir, console_level)

    # directories
    session_dir = logger.session_dir
    prompts_dir = session_dir / "prompts"
    responses_dir = session_dir / "responses"
    tools_dir = session_dir / "tools"
    errors_dir = session_dir / "errors"
    warnings_dir = session_dir / "warnings"
    artifacts_dir = session_dir / "artifacts"
    combined_log_path = session_dir / "session_combined.json"

    tool_result_dump_map: dict[str, str] = {}

    def dump_text(name: str, content: str) -> None:
        try:
            content_str = content if isinstance(content, str) else str(content)
            # combined jsonl
            with open(combined_log_path, 'a', encoding='utf-8') as cf:
                cf.write(json.dumps({"name": name, "timestamp": datetime.now().isoformat(), "content": content_str}, ensure_ascii=False) + "\n")
            # unified logger events
            if name.startswith("prompt_"):
                logger._log_structured('prompt', f"Prompt generated: {name}", name=name, content=content_str)
            elif name.startswith("response_"):
                logger._log_structured('response', f"Response received: {name}", name=name, content=content_str)
            elif name.startswith("tool_"):
                # tool_ æ–‡ä»¶ï¼šå…ˆä¸å¤„ç†ï¼Œç­‰ä¿å­˜æ–‡ä»¶åå†è°ƒç”¨ log_tool_execution
                pass
            elif name.startswith("error_"):
                logger._log_structured('error', f"Error: {name}", name=name, content=content_str)
            else:
                logger._log_structured('artifact', f"Artifact: {name}", name=name, content=content_str)
            # discrete files
            p = None
            if name.startswith("prompt_"):
                p = prompts_dir / name
            elif name.startswith("response_"):
                p = responses_dir / name
            elif name.startswith("tool_"):
                p = tools_dir / name
            elif name.startswith("error_"):
                p = errors_dir / name
            else:
                p = artifacts_dir / name
            p.parent.mkdir(parents=True, exist_ok=True)
            with open(p, 'w', encoding='utf-8') as f:
                f.write(content_str)
            
            # tool_ æ–‡ä»¶ï¼šä¿å­˜åè°ƒç”¨ log_tool_executionï¼Œä½¿ç”¨å·²ä¿å­˜çš„è·¯å¾„
            if name.startswith("tool_"):
                try:
                    tool_name = name.replace("tool_", "").replace(".json", "")
                    data = json.loads(content_str)
                    relative_path = str(p.relative_to(session_dir))
                    # ä¸ä¼  resultï¼Œé¿å…é‡å¤ä¿å­˜
                    logger.log_tool_execution(
                        tool_name=tool_name,
                        success=bool(data.get("success")),
                        result=None,  # result å·²ä¿å­˜åœ¨ tools/ ç›®å½•
                        execution_time=float(data.get("execution_time", 0.0)),
                        error=data.get("error"),
                        input_params=data.get("input_params") or data.get("parameters"),
                    )
                    # è®°å½•æ–‡ä»¶è·¯å¾„
                    tool_result_dump_map[tool_name] = relative_path
                except Exception as tool_log_error:
                    logger.warning(f"Failed logging tool execution for {name}: {tool_log_error}")
                    
        except Exception as e:
            logger.warning(f"Failed writing {name}: {e}")

    # session start meta
    logger.log_session_start({
        "session_id": session_id,
        "args": {"requirement": args.requirement, "target_app": args.target_app, "device_id": args.device_id, "max_rounds": args.max_rounds, "execute": args.execute},
        "paths": {"session_dir": str(session_dir)},
        "timestamps": {"started_at": datetime.now().isoformat()},
    })

    # mcp_execution_log: only records MCP tool invocations and their responses.
    # Note: We intentionally do not create execution_log.json anymore.
    #       Use mcp_execution_log.json to inspect MCP call history.
    mcp_exec_log_path = session_dir / "mcp_execution_log.json"
    mcp_log_entries = []  # Collect all entries in memory for proper JSON array format
    
    def append_mcp_log(*, tool: str, parameters: dict | None, success: bool, result_dump_path: str | None, phase: str, round_idx: int | None = None, error: str | None = None, exec_log: list | None = None, result_summary: dict | None = None) -> None:
        try:
            rec = {
                "phase": phase,
                "tool": tool,
                "parameters": parameters or {},
                "success": bool(success),
                "result_dump_path": result_dump_path or "",
                "timestamp": datetime.now().isoformat(),
            }
            if round_idx is not None:
                rec["round"] = int(round_idx)
            if error:
                rec["error"] = str(error)
            if exec_log:
                rec["exec_log"] = list(exec_log)  # âœ… è®°å½•æ‰§è¡Œæ—¥å¿—
            if result_summary:
                rec["result_summary"] = dict(result_summary)  # âœ… è®°å½•ç»“æœæ‘˜è¦
            
            # Append to in-memory list
            mcp_log_entries.append(rec)
            
            # Write complete JSON array (overwrite each time for proper format)
            with open(mcp_exec_log_path, 'w', encoding='utf-8') as f:
                json.dump(mcp_log_entries, f, ensure_ascii=False, indent=2)
            
            # Also write to combined log for convenience
            with open(combined_log_path, 'a', encoding='utf-8') as cf:
                cf.write(json.dumps({"name": "mcp_execution_log", **rec}, ensure_ascii=False) + "\n")
        except Exception:
            pass

    # execution_log.jsonl å·²ç§»é™¤ - æ— å®é™…ç”¨é€”
    # æ‰€æœ‰é‡è¦æ—¥å¿—å·²è®°å½•åœ¨ mcp_execution_log.json å’Œ session_combined.json ä¸­

    # config loaders
    def _load_yaml() -> dict:
        try:
            cfg = Path('only_test/config/framework_config.yaml')
            if not cfg.exists():
                return {}
            import yaml  # type: ignore
            with open(cfg, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f) or {}
        except Exception:
            return {}

    def _load_auto_close_limit_from_config() -> int:
        data = _load_yaml()
        return int(((data.get('recognition') or {}).get('ads') or {}).get('auto_close_limit', 3))

    def _load_history_window_from_config() -> int:
        data = _load_yaml()
        hv = ((data.get('llm') or {}).get('history_window')) or ((data.get('prompts') or {}).get('history_window'))
        try:
            return int(hv) if hv is not None else 10
        except Exception:
            return 10

    session_auto_close_limit = args.auto_close_limit if args.auto_close_limit is not None else _load_auto_close_limit_from_config()
    session_history_window = args.history_window if args.history_window is not None else _load_history_window_from_config()
    try:
        os.environ['ONLY_TEST_AUTO_CLOSE_LIMIT'] = str(session_auto_close_limit)
    except Exception:
        pass

    # MCP server and tools
    server = MCPServer(device_id=args.device_id)
    from only_test.lib.mcp_interface.device_inspector import DeviceInspector
    inspector = DeviceInspector(device_id=args.device_id)
    await inspector.initialize()
    for attr_name in dir(inspector):
        fn = getattr(inspector, attr_name, None)
        if callable(fn) and hasattr(fn, "_mcp_tool_info"):
            info = getattr(fn, "_mcp_tool_info")
            server.register_tool(MCPTool(name=info["name"], description=info["description"], parameters=info.get("parameters", {}), function=fn, category=info.get("category", "general")))

    # Informative note if not executing actions (plan-only mode)
    if not args.run_stepwise:
        logger.info("å½“å‰ä¸ºè§„åˆ’/æé—®æ¨¡å¼ï¼ˆæœªä¼  --run-stepwiseï¼‰ï¼Œä¸ä¼šæ‰§è¡Œæ¯è½® UI åŠ¨ä½œï¼›ä»…è®°å½• MCP å±å¹•åˆ†æä¸ LLM è¾“å‡ºã€‚")

    # restart target app
    try:
        _params_start = {"application": args.target_app, "force_restart": True}
        start_resp = await server.execute_tool("start_app", _params_start)
        _start_dict = start_resp.to_dict() if hasattr(start_resp, 'to_dict') else start_resp
        try:
            _tmp = dict(_start_dict); _tmp["input_params"] = _params_start
            dump_text("tool_start_app.json", json.dumps(_tmp, ensure_ascii=False, indent=2))
        except Exception:
            pass
        # Enforce fail-fast: exit when start_app reports failure (either top-level or nested result.success)
        start_success = bool(getattr(start_resp, 'success', False))
        try:
            res_obj = getattr(start_resp, 'result', None)
            if isinstance(res_obj, dict) and ('success' in res_obj):
                start_success = start_success and bool(res_obj.get('success'))
        except Exception:
            pass
        if start_success:
            try:
                append_mcp_log(tool="start_app", parameters=_params_start, success=True, result_dump_path="tools/tool_start_app.json", phase="init")
            except Exception:
                pass
        if not start_success:
            err_msg = None
            try:
                err_msg = getattr(start_resp, 'error', None)
                if not err_msg and isinstance(getattr(start_resp, 'result', None), dict):
                    err_msg = start_resp.result.get('error')
            except Exception:
                pass
            err_text = f"å¯åŠ¨åº”ç”¨å¤±è´¥{(' - ' + err_msg) if err_msg else ''}"
            dump_text("error_start_app.txt", err_text)
            try:
                logger._log_structured('error', 'start_app failed', name='start_app', content=err_text)
            except Exception:
                pass
            try:
                append_mcp_log(tool="start_app", parameters=_params_start, success=False, result_dump_path="tools/tool_start_app.json", phase="init", error=err_text)
            except Exception:
                pass
            import sys as _sys
            _sys.exit(3)
    except Exception as hook_e:
        dump_text("error_start_app_exception.txt", str(hook_e))
        try:
            append_mcp_log(tool="start_app", parameters={"application": args.target_app, "force_restart": True}, success=False, result_dump_path=None, phase="init", error=str(hook_e))
        except Exception:
            pass
        import sys as _sys
        _sys.exit(3)

    # Utilities for JSON extraction
    def _sanitize_json_str(s: str) -> str:
        import re
        s = s.strip()
        if s.startswith('```json'):
            s = s[7:]
        if s.startswith('```'):
            s = s[3:]
        if s.endswith('```'):
            s = s[:-3]
        s = re.sub(r"(^|\n)\s*//.*?(?=\n|$)", "\n", s)
        s = re.sub(r",\s*([}\]])", r"\1", s)
        return s.strip()

    def _split_json_objects_by_braces(s: str) -> list:
        s = _sanitize_json_str(s)
        objs, depth, start = [], 0, None
        for i, ch in enumerate(s):
            if ch == '{':
                if depth == 0:
                    start = i
                depth += 1
            elif ch == '}':
                depth = max(0, depth-1)
                if depth == 0 and start is not None:
                    objs.append(s[start:i+1]); start = None
        if not objs:
            return [s] if s else []
        return objs

    def _safe_json_load(candidate: str):
        try:
            return True, json.loads(_sanitize_json_str(candidate))
        except Exception:
            return False, None

    def _pick_best_candidate(cands: list, required_keys: list) -> dict:
        best = None
        for c in cands:
            ok, obj = _safe_json_load(c)
            if not ok:
                continue
            if isinstance(obj, dict):
                if any((k in obj) for k in (required_keys or [])):
                    return obj
                if best is None:
                    best = obj
        return best or {}

    def _extract_json_robust(content: str, kind: str = "generic", required_keys: list | None = None) -> dict:
        req = list(required_keys or [])
        cands = _split_json_objects_by_braces(content or "")
        obj = _pick_best_candidate(cands, req)
        return obj if isinstance(obj, dict) else {}

    def _build_selector_pool_for_pkg(screen_resp, target_pkg: str):
        els = (screen_resp.result or {}).get('elements', []) if getattr(screen_resp, 'success', False) else []
        rids, texts = [], []
        for e in els:
            pkg = (e.get('package') or '').strip()
            rid = (e.get('resource_id') or '').strip()
            tx = (e.get('text') or '').strip()
            if (rid and rid.startswith(f"{target_pkg}:id/")) or pkg == target_pkg:
                if rid:
                    rids.append(rid)
                if tx:
                    texts.append(tx)
        def _dedup(seq):
            seen, out = set(), []
            for x in seq:
                if x not in seen:
                    seen.add(x); out.append(x)
            return out
        return {'resource_ids': _dedup(rids), 'texts': _dedup(texts)}

    def _format_selector_pool(pool: dict) -> str:
        def fmt(name: str, vals: list) -> str:
            vals = vals or []
            return f"- {name}({len(vals)}): " + (", ".join([str(v) for v in vals]) if vals else "[]")
        return "\n".join([fmt('resource_id', pool.get('resource_ids', [])), fmt('text', pool.get('texts', []))])

    def _build_screen_summary_json(screen_resp, target_pkg: str, max_items: int = 30) -> str:
        try:
            scr = screen_resp.to_dict() if hasattr(screen_resp, 'to_dict') else screen_resp
            res = (scr or {}).get('result', scr or {})
            els = (res or {}).get('elements', []) or []
            def _in_target(e: dict) -> bool:
                rid = (e.get('resource_id') or '').strip()
                pkg = (e.get('package') or '').strip()
                return bool((rid and rid.startswith(f"{target_pkg}:id/")) or (pkg == target_pkg))
            filtered = []
            for e in els:
                if not isinstance(e, dict) or not _in_target(e):
                    continue
                rid = (e.get('resource_id') or '').strip()
                tx = (e.get('text') or '').strip()
                if not rid and not tx:
                    continue
                filtered.append({'text': tx, 'resource_id': rid, 'class_name': (e.get('class_name') or '')})
            summary = {
                'current_app': (res or {}).get('current_app') or (scr or {}).get('current_app') or 'unknown',
                'current_page': (res or {}).get('current_page') or 'unknown',
                'page': (res or {}).get('page') or 'unknown',
                'total_elements': int((res or {}).get('total_elements', 0)),
                'clickable_elements': int((res or {}).get('clickable_elements', 0)),
                'element_analysis': {'has_text': int(((res or {}).get('element_analysis') or {}).get('has_text', 0))},
                'media_playing': bool((res or {}).get('media_playing', False)),
                'elements': filtered[:max_items],
            }
            return json.dumps(summary, ensure_ascii=False, indent=2)
        except Exception:
            return "{}"

    # LLM plan
    logger.set_phase("plan")
    # initial screen for planning
    _params_plan = {"include_elements": True, "clickable_only": True, "auto_close_limit": session_auto_close_limit}
    initial_screen = await server.execute_tool("get_current_screen_info", _params_plan)
    _initial_dict = initial_screen.to_dict() if hasattr(initial_screen, 'to_dict') else initial_screen
    try:
        _id2 = dict(_initial_dict); _id2["input_params"] = _params_plan
        dump_text("tool_get_current_screen_info_plan.json", json.dumps(_id2, ensure_ascii=False, indent=2))
    except Exception:
        pass
    try:
        append_mcp_log(tool="get_current_screen_info", parameters=_params_plan, success=bool(getattr(initial_screen,'success', True)), result_dump_path="tools/tool_get_current_screen_info_plan.json", phase="plan")
    except Exception:
        pass

    # few-shot examples (embed contents)
    examples = []
    try:
        golden_json_path = Path('only_test/testcases/generated/golden_example_airtest_record.json')
        golden_code_path = Path('only_test/testcases/python/example_airtest_record.py')
        if golden_code_path.exists():
            examples.append({'file': str(golden_code_path), 'content': golden_code_path.read_text(encoding='utf-8')})
        if golden_json_path.exists():
            examples.append({'file': str(golden_json_path), 'content': golden_json_path.read_text(encoding='utf-8')})
    except Exception:
        examples = []

    def _render_examples(examples: list) -> str:
        try:
            import os as _os
            rendered = []
            for e in examples or []:
                if not isinstance(e, dict):
                    continue
                fname = _os.path.basename(e.get('file',''))
                code = (e.get('content') or '').strip()
                if len(code) > 6000:
                    code = code[:6000] + "\n# ...(å†…å®¹å·²æˆªæ–­)"
                if fname and code:
                    rendered.append(f"{fname}:\n{code}")
            return ("å¾€æœŸç”¨ä¾‹ç¤ºä¾‹ï¼ˆä»…å‚è€ƒåŠ¨ä½œæ€è·¯ï¼‰ [[[ START ]]]  :\n\n" + "\n\n".join(rendered) + " [[[ END ]]]") if rendered else ""
        except Exception:
            return ""

    plan_prompt = (
        "# ä»…è¾“å‡ºä¸¥æ ¼JSONï¼Œä¸è¦ä½¿ç”¨Markdownã€‚\n\n"
        "ä½ æ˜¯ Only-Test çš„ç”¨ä¾‹è§„åˆ’åŠ©æ‰‹ã€‚è¯·å…ˆç»™å‡ºé«˜å±‚æ¬¡è®¡åˆ’ï¼ˆä¸æŒ‘å…·ä½“selectorï¼‰ï¼Œå†ç”±åç»­æ­¥éª¤åŸºäºå½“å‰XMLé€‰æ‹©å™¨æ‰§è¡Œã€‚\n\n"
        f"æµ‹è¯•ç›®æ ‡: {args.requirement}\n\n"
        + _render_examples(examples) + ("\n\n" if _render_examples(examples) else "") +
        "[[[ END ]]], æ³¨æ„ï¼šè®¡åˆ’é˜¶æ®µç¦æ­¢ç¼–é€ ä»»ä½• resource_id/text å€¼ï¼›å…·ä½“selector ç”±åç»­æ­¥éª¤ä»å½“å‰XMLçš„å¯é€‰åˆ—è¡¨ä¸­é€‰æ‹©ã€‚\n\n"
        "ã€ä¸¥ç¦æ­¥éª¤ã€‘åº”ç”¨å¯åŠ¨/é‡å¯/å¹¿å‘Šå…³é—­å·²ç”±æ¡†æ¶è‡ªåŠ¨å®Œæˆï¼Œæµ‹è¯•ä»åº”ç”¨å·²å¯åŠ¨åå¼€å§‹ï¼š\n"
        "- ç¦æ­¢: restart, launch, close_ads, start_app\n"
        "- å…è®¸: click, input, press, wait_for_elements, wait, assert, swipe, click_with_bias, wait_for_disappearance\n"
        "- ç‰¹ä¾‹: ä»…å½“æµ‹è¯•éœ€æ±‚æ˜ç¡®è¦æ±‚\"é‡å¯åº”ç”¨\"\"é€€å‡ºé‡è¿›\"æ—¶æ‰å¯ä½¿ç”¨ restart\n\n"
        "æ‰€æœ‰å¯ç”¨åŠ¨ä½œç±»åˆ«ï¼ˆåç»­æ­¥éª¤ä¼šç”¨åˆ°ï¼‰ï¼šclick, input, press (ä»…æ”¯æŒENTER/BACK/HOME/MENU), wait_for_elements, wait, assert, swipe, click_with_bias, wait_for_disappearanceã€‚\n"
        f"è¾“å‡ºJSONæ ¼å¼ï¼ˆå¿…é¡»åŒ…å« keyword å’Œ max_roundsï¼›å…¶ä¸­ max_rounds ä¸ºæœ¬è®¡åˆ’æ‰€éœ€çš„å®é™…è½®æ•°ï¼Œä¸”ä¸å¾—è¶…è¿‡ä¸Šé™ {args.max_rounds}ï¼‰ï¼š{{\n"
        "  \"plan_id\": \"plan_YYYYmmdd_HHMMSS\",\n"
        "  \"objective\": \"...\",\n"
        "  \"keyword\": \"(æœ¬ç”¨ä¾‹å…³é”®è¯­ä¹‰æ ‡è¯†)\",\n"
        "  \"max_rounds\": 4,\n"
        "  \"steps\": [\n"
        "    {\"intent\": \"...\", \"action\": \"click\"}\n"
        "  ]\n"
        "}\n"
        "æ³¨æ„ï¼šsteps ä¸­ä¸è¦åŒ…å« restart/launch/close_ads ç­‰æ­¥éª¤ï¼\n"
        "æ¡†æ¶å·²è‡ªåŠ¨å®Œæˆçš„æ“ä½œï¼ˆç¦æ­¢åœ¨è®¡åˆ’ä¸­é‡å¤ï¼‰ï¼šå¯åŠ¨åº”ç”¨ã€å…³é—­å¹¿å‘Šã€ç­‰å¾…åˆå§‹åŒ–ã€‚\n\n"
    )
    dump_text("prompt_plan.txt", plan_prompt)
    llm = LLMClient()
    plan_msgs = [{"role": "system", "content": "You are Only-Test LLM. Output strict JSON only."}, {"role": "user", "content": plan_prompt}]
    plan_resp = llm.chat_completion(plan_msgs, temperature=0.1, max_tokens=800)
    dump_text("response_plan.txt", plan_resp.content or "")
    
    # æ‰“å° Plan Response åˆ°å‘½ä»¤è¡Œ
    logger.info("=" * 80)
    logger.info("PLAN RESPONSE:")
    if plan_resp.content:
        logger.info(plan_resp.content[:500] + ("...\n(å†…å®¹å·²æˆªæ–­ï¼Œå®Œæ•´å†…å®¹è§ response_plan.txt)" if len(plan_resp.content) > 500 else ""))
    else:
        logger.warning("Plan response is empty!")
    logger.info("=" * 80)
    
    plan_json = {}
    if plan_resp.success:
        try:
            plan_json = _extract_json_robust(plan_resp.content, kind="plan", required_keys=["plan_id","objective","steps"]) or {}
        except Exception as e:
            dump_text("error_parse_plan.txt", f"{e}\n\nRAW:\n{plan_resp.content}")
    plan_json.setdefault("plan_id", "plan_default")
    plan_json.setdefault("objective", args.requirement)
    plan_json.setdefault("keyword", "")
    plan_json.setdefault("steps", [])
    dump_text("parsed_plan.json", json.dumps(plan_json, ensure_ascii=False, indent=2))
    
    # è½®æ¬¡è®¡ç®—ï¼šä¼˜å…ˆé‡‡ç”¨è®¡åˆ’é‡Œçš„ max_roundsï¼Œå…¶æ¬¡é€€å›åˆ° CLI ä¸Šé™ï¼›ä¸¤è€…å–è¾ƒå°å€¼ï¼›å¹¶è®¾ç½®ç»å¯¹ä¸Šé™ 20 ä¸ä¸‹é™ 1
    cap_cli = int(args.max_rounds)
    planned_rounds = None
    try:
        if 'max_rounds' in plan_json:
            planned_rounds = int(plan_json.get('max_rounds'))
    except Exception:
        planned_rounds = None

    if planned_rounds is None or planned_rounds <= 0:
        total_rounds = max(1, min(cap_cli, 20))
    else:
        total_rounds = max(1, min(planned_rounds, cap_cli, 20))

    # æç¤ºï¼šä»æ­¤å¤„å¼€å§‹ä»…è®°å½• MCP å·¥å…·è°ƒç”¨åˆ° mcp_execution_log.jsonï¼ˆplan å…ƒæ•°æ®ä¸å†å†™å…¥ execution_logï¼‰
    generated_steps = []
    step_responses: list[dict] = []

    # ============================================================
    # EXECUTION é˜¶æ®µä¸»å¾ªç¯ - æ¯è½®æµç¨‹è¯´æ˜
    # ============================================================
    # 1. å±å¹•æŠ“å–: get_current_screen_info (target_appè¿‡æ»¤)
    # 2. Promptæ„å»º: è®¡åˆ’+selectoræ¸…å•+ä¸ŠNè½®è¾“å‡º+æŒ‡å¯¼
    # 3. LLMè°ƒç”¨: è¿”å› current_action æˆ– tool_request
    # 4. åˆ·æ–°æœºåˆ¶(è‹¥tool_request): è‡ªåŠ¨refresh â†’ äºŒæ¬¡æé—®
    # 5. è®°å½•æ—¥å¿—: execution_log.json + æ–‡ä»¶åˆ†ç¦»å­˜å‚¨
    # ============================================================
    for round_idx in range(1, total_rounds + 1):
        logger.set_phase("execution", current_round=round_idx, max_rounds=total_rounds)
        _params_round = {"include_elements": True, "clickable_only": True, "auto_close_limit": session_auto_close_limit}
        screen = await server.execute_tool("get_current_screen_info", _params_round)
        dump_text(f"tool_get_current_screen_info_round_{round_idx}.json", json.dumps((screen.to_dict() if hasattr(screen,'to_dict') else screen), ensure_ascii=False, indent=2))
        try:
            append_mcp_log(tool="get_current_screen_info", parameters=_params_round, success=bool(getattr(screen,'success', True)), result_dump_path=f"tools/tool_get_current_screen_info_round_{round_idx}.json", phase="execution", round_idx=round_idx)
        except Exception:
            pass

        # examples for rounds
        examples_r = []
        try:
            py_dir = Path('only_test/testcases/python')
            if py_dir.exists():
                for pf in sorted(py_dir.glob('*.py'))[:2]:
                    examples_r.append({'file': str(pf), 'content': pf.read_text(encoding='utf-8')})
        except Exception:
            examples_r = []
        examples_block = _render_examples(examples_r)

        # last-N responses block
        def _last_n_responses(n: int) -> str:
            if not step_responses:
                return "(æ— )"
            items = step_responses[-max(1, int(n)) :]
            out = []
            for it in items:
                txt = (it.get('response') or '').strip()
                if len(txt) > 1200:
                    txt = txt[:1200] + "\n...TRUNCATED..."
                out.append(f"round={it.get('round')}:\n{txt}")
            return "\n\n".join(out)

        # current expected plan step
        def _current_plan_step(idx: int) -> str:
            try:
                steps = (plan_json or {}).get('steps') or []
                if not isinstance(steps, list) or not steps:
                    return "{}"
                i = max(0, min(len(steps)-1, idx-1))
                return json.dumps(steps[i], ensure_ascii=False)
            except Exception:
                return "{}"

        # ============================================================
        # Prompt æ„å»ºå‡½æ•° - æ¶ˆé™¤ step_prompt å’Œ step_prompt2 çš„é‡å¤
        # ============================================================
        def _build_step_prompt(screen_data, last_note_text: str) -> str:
            """ç»Ÿä¸€çš„ step prompt æ„å»ºå‡½æ•°ï¼Œæ”¯æŒé¦–æ¬¡æé—®å’Œåˆ·æ–°åäºŒæ¬¡æé—®"""
            return (
                "# ä»…è¾“å‡ºä¸¥æ ¼JSONã€‚ä¸¥ç¦Markdownã€‚åªè¾“å‡ºä¸€ä¸ª JSON å¯¹è±¡ã€‚\n\n"
                + last_note_text +
                f"æµ‹è¯•ç›®æ ‡: {args.requirement}\n\n"
                f"æ€»ä½“è®¡åˆ’: {json.dumps(plan_json, ensure_ascii=False)}\n\n"
                f"ç›®æ ‡åº”ç”¨åŒ…: {args.target_app}\n\n"
                "é€‰æ‹©å™¨èŒƒå›´è§„åˆ™ï¼š\n"
                f"- ä»…å…è®¸ç›®æ ‡åº”ç”¨å…ƒç´ ï¼šresource_id ä»¥ '{args.target_app}:id/' å¼€å¤´ï¼Œæˆ– package == '{args.target_app}'ï¼›\n"
                "- ç³»ç»Ÿå¯¹è¯ç™½åå•ï¼šandroid, com.android.permissioncontroller, com.google.android.permissioncontroller, com.android.packageinstaller, com.android.systemuiã€‚\n\n"
                "å¯åŠ¨/å¹¿å‘Šå¤„ç†ï¼šå¯åŠ¨/é‡æ–°å¯åŠ¨åº”ç”¨/å…³é—­å¯åŠ¨åº”ç”¨å¹¿å‘Šå·²ç”±æ¡†æ¶è‡ªåŠ¨å¤„ç†ï¼Œç¦æ­¢ç”Ÿæˆä»»ä½• close_ads/å…³é—­å¹¿å‘Š çš„åŠ¨ä½œæˆ–æ­¥éª¤ã€‚\n\n"
                "ç›®æ ‡åº”ç”¨å¯é€‰é€‰æ‹©å™¨ï¼ˆJSON æ¦‚è§ˆï¼Œå·²è¿‡æ»¤æ— æ•ˆæ§ä»¶ï¼‰ï¼š\n"
                f"{_build_screen_summary_json(screen_data, args.target_app, max_items=30)}\n\n"
                + (examples_block + "\n\n" if examples_block else "") +
                f"ä¸Š {session_history_window} è½®è¾“å‡ºæ˜¯ï¼š\n{_last_n_responses(session_history_window)}\n\n"
                f"ä½ çš„ä»»åŠ¡æ˜¯ step[{round_idx}] = {_current_plan_step(round_idx)}\n\n"
                f"ä¹‹å‰æ­¥éª¤(æ‘˜è¦)[[[ START ]]]: {json.dumps(generated_steps, ensure_ascii=False)}[[[ END ]]]\n\n"
                "==== è¾“å‡ºè¦æ±‚ ====\n"
                "ã€é»˜è®¤è¾“å‡ºã€‘ç›´æ¥è¿”å›æ“ä½œå†³ç­–ï¼Œæ ¹æ®ä¸Šè¿°å¯ç”¨é€‰æ‹©å™¨å’Œå½“å‰ä»»åŠ¡ç”Ÿæˆå…·ä½“åŠ¨ä½œã€‚\n\n"
                "ã€æ ‡å‡†è¾“å‡ºæ ¼å¼ã€‘- 99%æƒ…å†µä½¿ç”¨æ­¤æ ¼å¼ï¼š\n"
                "{\n"
                "  \"analysis\": {\n"
                "    \"current_page_type\": \"å½“å‰é¡µé¢ç±»å‹\",\n"
                "    \"available_actions\": [\"click\",\"input\",\"press\",\"wait_for_elements\",\"wait\",\"restart\",\"launch\",\"assert\",\"swipe\"],\n"
                "    \"reason\": \"åˆ†æä¾æ®\",\n"
                "    \"next_round_hint\": \"(å¯é€‰)ç»™ä¸‹ä¸€è½®çš„ç®€çŸ­æç¤ºï¼Œå¸®åŠ©ä¿æŒè®¡åˆ’è¿è´¯æ€§\"\n"
                "  },\n"
                "  \"current_action\": {  // â† è¿™ä¸ªä¼šè¢«MCPç«‹å³æ‰§è¡Œ\n"
                "    \"action\": \"click|input|press|wait_for_elements|wait|restart|launch|assert|swipe\",\n"
                "    \"reason\": \"é€‰æ‹©æ­¤åŠ¨ä½œçš„ä¾æ®(æ¯”å¦‚: \\\"å¾€æœŸç”¨ä¾‹ç¤ºä¾‹\\\"ä¸­æœ‰ç›¸ä¼¼ç”¨ä¾‹, ç›´æ¥éµå¾ªä½¿ç”¨ç›¸åŒæ­¥éª¤çš„åŠ¨ä½œ)\",\n"
                "    \"target\": {\n"
                "      // click/input/wait_for_elements ä½¿ç”¨ priority_selectors\n"
                "      \"priority_selectors\": [{\"resource_id\": \"com.app:id/xxx\"}],\n"
                "      // press ä½¿ç”¨ keyevent å­—æ®µï¼Œä»…é™ENTER/BACK/HOME/MENUå››ä¸ªå€¼\n"
                "      \"keyevent\": \"ENTER|BACK|HOME|MENU (ä»…pressåŠ¨ä½œï¼Œç¦æ­¢å…¶å®ƒå€¼)\"\n"
                "    },\n"
                "    \"data\": \"è¾“å…¥æ–‡æœ¬(ä»…inputåŠ¨ä½œéœ€è¦)\",\n"
                "    \"wait_after\": 0.8,\n"
                "    \"expected_result\": \"é¢„æœŸç»“æœæè¿°\"\n"
                "  },\n"
                "  \"evidence\": {\"screen_hash\": \"...\"}\n"
                "}\n\n"
                "ã€é€‰æ‹©å™¨ç”Ÿæˆè§„åˆ™ã€‘ï¼š\n"
                "- priority_selectors å¿…é¡»ä»ä¸Šè¿°ã€Œç›®æ ‡åº”ç”¨å¯é€‰é€‰æ‹©å™¨æ¸…å•ã€ä¸­ç²¾ç¡®å¤åˆ¶ resource_id æˆ– text\n"
                "- è‹¥æ¸…å•ä¸­æ— å®Œå…¨åŒ¹é…çš„æ§ä»¶ï¼Œä½¿ç”¨ wait_for_elements ç­‰å¾…æˆ– swipe æ»šåŠ¨æŸ¥æ‰¾\n"
                "- ç¦æ­¢ç¼–é€ ä¸å­˜åœ¨çš„ resource_id/text\n\n"
                "ã€å­—æ®µè¯´æ˜ã€‘ï¼š\n"
                "- current_action: æœ¬è½®è¦ç«‹å³æ‰§è¡Œçš„åŠ¨ä½œï¼ˆå¿…é¡»å­—æ®µï¼‰\n"
                "- next_round_hint: ç»™ä¸‹ä¸€è½®çš„æç¤ºï¼Œé¿å…è§„åˆ’æ¼‚ç§»ï¼ˆå¯é€‰å­—æ®µï¼Œä¸ä¼šè¢«æ‰§è¡Œï¼‰\n\n"
                "ã€åŠ¨ä½œå‚æ•°è¯´æ˜ã€‘ï¼š\n"
                "- click/input/wait_for_elements: éœ€è¦ priority_selectors å®šä½å…ƒç´ \n"
                "- press: éœ€è¦ target.keyevent å­—æ®µï¼Œä»…æ”¯æŒä»¥ä¸‹4ä¸ªæŒ‰é”®ï¼ˆç¦æ­¢ä½¿ç”¨å…¶å®ƒå€¼ï¼‰ï¼š\n"
                "  * ENTER - å›è½¦é”®ï¼Œç”¨äºæœç´¢ç¡®è®¤ã€è¾“å…¥æäº¤\n"
                "  * BACK - è¿”å›é”®ï¼Œç”¨äºé€€å‡ºå½“å‰é¡µé¢\n"
                "  * HOME - ä¸»å±é”®ï¼Œç”¨äºè¿”å›æ¡Œé¢\n"
                "  * MENU - èœå•é”®ï¼Œç”¨äºæ‰“å¼€é€‰é¡¹èœå•\n"
                "  ç¤ºä¾‹ï¼š{\"action\": \"press\", \"target\": {\"keyevent\": \"ENTER\"}}\n"
                "  è½¬æ¢ä¸º Python: keyevent('ENTER')\n"
                "  [!] å…¶å®ƒæŒ‰é”®ï¼ˆå¦‚RECENT/DEL/DPADç­‰ï¼‰ä¸æ”¯æŒï¼Œè¯·å‹¿ä½¿ç”¨\n"
                "- swipe: éœ€è¦ target ä¸­çš„ start_px/end_px åæ ‡\n"
                "- wait/restart/launch: ä¸éœ€è¦ target\n\n"
                "ã€ç‰¹æ®Šæƒ…å†µã€‘ä»…å½“åŒæ—¶æ»¡è¶³ä»¥ä¸‹3ä¸ªæ¡ä»¶æ—¶æ‰å¯è¿”å› tool_requestï¼ˆæå°‘ä½¿ç”¨ï¼‰ï¼š\n"
                "1. ä¸Šè¿°é€‰æ‹©å™¨æ¸…å•å®Œå…¨ä¸ºç©º æˆ– å…¨æ˜¯æ— å…³æ§ä»¶\n"
                "2. å½“å‰è½®æ¬¡å°šæœªä½¿ç”¨è¿‡ tool_request\n"
                "3. ç¡®å®éœ€è¦é‡æ–°åˆ†æå±å¹•è·å–æœ€æ–°å…ƒç´ \n"
                "æ ¼å¼: {\"tool_request\": {\"name\": \"analyze_current_screen\", \"params\": {}, \"reason\": \"å…·ä½“åŸå› \"}}\n"
            )

        # inject guidance from previous round
        last_note = ""
        try:
            notes = []
            if generated_steps and isinstance(generated_steps[-1], dict):
                if generated_steps[-1].get('invalid_note'):
                    notes.append(generated_steps[-1]['invalid_note'])
                if generated_steps[-1].get('next_round_hint'):
                    notes.append("ä¸Šä¸€è½®æç¤º: " + str(generated_steps[-1]['next_round_hint']))
            last_note = ("\n\n".join(notes) + "\n\n") if notes else ""
        except Exception:
            last_note = ""

        # ============================================================
        # Step Prompt æ„å»º - æ¯è½®çš„ LLM è¾“å…¥
        # ============================================================
        # å…³é”®è¦ç´  (scopeä»å®½åˆ°ä¸¥):
        #  â‘  é«˜å±‚è®¡åˆ’: plan_json (step[round_idx])
        #  â‘¡ Selector å¯é€‰: JSON æ¦‚è§ˆ + æ¸…å• (resource_id/text å»é‡)
        #  â‘¢ ä¸ŠNè½®ä¸Šä¸‹æ–‡: å…ˆå‰ step_responses ä¸­æœ€è¿‘ N è½® çš„ response
        #  â‘£ å‰ä¸€è½®åé¦ˆ: invalid_note (if å¤±è´¥) + next_round_hint (if æŒ‡å¯¼)
        #  â‘¤ çº¦æŸè§„åˆ™: åªç”¨æä¾›çš„ selector; æ— åˆ¹ä¼˜å…ˆ wait/swipe; æœ€å¤šåˆ·æ–°1æ¬¡ tool_request
        #
        # ä¸‰ç§è¾“å‡ºä¹‹ä¸€ (LLM å¿…é¡»ä»…è¾“å‡ºä¸€ä¸ª JSON):
        #  A. tool_request: {"tool_request": {"name": "analyze_current_screen", ...}}
        #  B. current_action: {"analysis": {...}, "current_action": {...}, "evidence": {...}}
        # ============================================================
        step_prompt = _build_step_prompt(screen, last_note)
        dump_text(f"prompt_step_{round_idx}.txt", step_prompt)
        msgs = [{"role": "system", "content": "You are Only-Test LLM. Output strict JSON only. Do not use markdown fences."}, {"role": "user", "content": step_prompt}]
        resp = llm.chat_completion(msgs, temperature=0.2, max_tokens=800)
        dump_text(f"response_step_{round_idx}.txt", resp.content or "")
        
        # æ‰“å° Step Response åˆ°å‘½ä»¤è¡Œ
        logger.info("-" * 80)
        logger.info(f"ğŸ”„ STEP {round_idx} RESPONSE:")
        if resp.content:
            preview = resp.content[:300] + ("...\n(æˆªæ–­)" if len(resp.content) > 300 else "")
            logger.info(preview)
        else:
            logger.warning(f"Step {round_idx} response is empty!")
        logger.info("-" * 80)
        
        try:
            step_responses.append({"round": round_idx, "response": (resp.content or "")})
        except Exception:
            pass

        # parse and remember guidance
        step_json = {}
        try:
            step_json = _extract_json_robust(resp.content, kind="step", required_keys=["current_action", "tool_request"]) or {}
        except Exception as e:
            dump_text(f"error_parse_step_{round_idx}.txt", f"{e}\n\nRAW:\n{resp.content}")
            continue
        try:
            # æå– next_round_hintï¼ˆå¯èƒ½åœ¨é¡¶å±‚æˆ– analysis ä¸­ï¼‰
            hint = step_json.get('next_round_hint') or ((step_json.get('analysis') or {}).get('next_round_hint'))
            if hint:
                step_json['next_round_hint'] = hint
        except Exception:
            pass

        generated_steps.append(step_json)
        
        # ============================================================
        # è‡ªåŠ¨åˆ·æ–°æœºåˆ¶ï¼šé¿å…é¦–å±æ¼‚ä½æµªè´¹å›åˆ
        # ============================================================
        # å¸¸è§„ï¼šä¸æ˜¯æ¯è½®è½®éƒ½éœ€è¦ refreshï¼ˆä¸æµªè´¹ï¼‰ã€‚åªåœ¨ LLM è¿”å› tool_request æ—¶æ‰§è¡Œ
        # æ‰§è¡Œæ­¥éª¤ï¼š
        #  1. æ£€æŸ¥ step_json æ˜¯å¦æœ‰ tool_request å­—æ®µ
        #  2. æ‰§è¡Œå±å¹•åˆ·æ–° (tool_name='analyze_current_screen')
        #  3. ä¿å­˜åˆ·æ–°åçš„å±å¹•ä¸º _refresh.json
        #  4. è®°å½•åˆ° execution_log: {"status": "tool_request_executed", "refresh_used": true}
        #  5. ç¬¬äºŒæ¬¡æé—®: ç”¨åˆ·æ–°åçš„å±å¹•é‡æ–°æ„å»º promptã€å¬å”¤ LLMã€è§£æç»“æœ
        #
        # äºŒæ¬¡æé—®çš„ä½œç”¨ï¼š
        #  - ç¬¬ä¸€æ¬¡ prompt çš„ selector æ¸…å•å¯èƒ½æ˜¯å¹¿å‘Š/loadingã€‚åˆ·æ–°åæœ‰çœŸå®çš„å…ƒç´ 
        #  - LLM ç¬¬äºŒæ¬¡æé—®æ—¶æœ‰æ›´å¤š current_action çš„é€‰æ‹©ï¼Œæé«˜æˆåŠŸç‡
        #  - ä¸€æ—¦äºŒæ¬¡æé—®ä¾ç„¶è¿”å› tool_requestï¼Œè®°å½• "invalid_note"ï¼Œç»§ç»­ä¸‹ä¸€è½®
        #
        # é˜»ã€å…³é—­æˆ–ä¿®æ”¹æ­¤æœºåˆ¶ï¼š
        #  - åˆ é™¤æ•´ä¸ª if å—ä»£ç : æ—¥å¿—ä¸­ä¸ä¼šè®°å½• refresh å›åˆ
        #  - æé«˜é˜ˆå€¾: ä¿®æ”¹ prompt ä¸­çš„"ä¼˜å…ˆ wait/swipe" æªè¾ä¸º"åº•ä¸¢æ”¶æ‰€æœ‰ wait/swipeï¼ˆæ— selectorï¼‰"
        #  - é€’ä¸Šä¸€æ¬¡æ±‚æƒ…ï¼šä¿®æ”¹ "max_rounds" æˆ– --max-rounds å‚æ•°2ï¼ˆå®éªŒsolue æ— æ±‚æƒ…ï¼‰
        # ============================================================
        try:
            if isinstance(step_json, dict) and isinstance(step_json.get('tool_request'), dict):
                tr = step_json['tool_request']
                tr_name = (tr.get('name') or '').strip()
                tr_params = tr.get('params') if isinstance(tr.get('params'), dict) else {}
                if tr_name == 'analyze_current_screen':
                    _params_tr = {"include_elements": True, "clickable_only": True, "auto_close_limit": session_auto_close_limit}
                    # allow prompt-provided params to override defaults safely
                    _params_tr.update({k:v for k,v in tr_params.items() if k in ("include_elements","clickable_only","auto_close_limit")})
                    tr_resp = await server.execute_tool("get_current_screen_info", _params_tr)
                    dump_text(f"tool_get_current_screen_info_round_{round_idx}_refresh.json", json.dumps((tr_resp.to_dict() if hasattr(tr_resp,'to_dict') else tr_resp), ensure_ascii=False, indent=2))
                    # Re-prompt once with refreshed screen info
                    try:
                        step_prompt2 = _build_step_prompt(tr_resp, last_note)
                        dump_text(f"prompt_step_{round_idx}_refresh.txt", step_prompt2)
                        msgs2 = [{"role": "system", "content": "You are Only-Test LLM. Output strict JSON only. Do not use markdown fences."}, {"role": "user", "content": step_prompt2}]
                        resp2 = llm.chat_completion(msgs2, temperature=0.2, max_tokens=800)
                        dump_text(f"response_step_{round_idx}_refresh.txt", resp2.content or "")
                        
                        # æ‰“å° Refresh Response åˆ°å‘½ä»¤è¡Œ
                        logger.info("-" * 80)
                        logger.info(f"ğŸ”„ STEP {round_idx} REFRESH RESPONSE (äºŒæ¬¡æé—®):")
                        if resp2.content:
                            preview = resp2.content[:300] + ("...\n(æˆªæ–­)" if len(resp2.content) > 300 else "")
                            logger.info(preview)
                        else:
                            logger.warning(f"Step {round_idx} refresh response is empty!")
                        logger.info("-" * 80)
                        
                        try:
                            step_responses.append({"round": round_idx, "response": (resp2.content or "")})
                        except Exception:
                            pass
                        ok2, obj2 = _safe_json_load(resp2.content or "")
                        if ok2 and isinstance(obj2, dict) and not obj2.get('tool_request'):
                            step_json = obj2
                    except Exception:
                        pass
                else:
                    # Unknown tool request type; just log warning
                    logger.warning(f"Unsupported tool request: {tr_name} at round {round_idx}")
        except Exception as e:
            logger.warning(f"Auto-refresh handling failed at round {round_idx}: {e}")
        
        # ============================================================
        # ç§»é™¤é˜²å¾¡æ€§æ£€æŸ¥ - è®© MCP é€šè¿‡å®é™…æ‰§è¡Œæ¥éªŒè¯æœ‰æ•ˆæ€§
        # å³ä½¿ current_action çœ‹èµ·æ¥æ— æ•ˆï¼Œä¹Ÿäº¤ç»™ MCP å¤„ç†å¹¶è¿”å›çœŸå®ç»“æœ
        # ============================================================

        # ============================================================
        # æ‰§è¡Œå®é™…çš„ UI åŠ¨ä½œ
        # ============================================================
        # ç»Ÿä¸€ä½¿ç”¨ execute_step_json å·¥å…·
        # - ç›´æ¥ä¼ å…¥ LLM è¾“å‡ºçš„ current_action
        # - MCPå±‚è´Ÿè´£éªŒè¯æœ‰æ•ˆæ€§å¹¶è¿”å›è¯¦ç»†ç»“æœ
        # - æ—¥å¿—ç»Ÿä¸€å‘½åä¸º mcp_execute_step_json.json
        # ============================================================
        action_result = None
        if isinstance(step_json, dict) and isinstance(step_json.get('current_action'), dict):
            current_action = step_json['current_action']
            action_type = current_action.get('action', '')
            
            # è·³è¿‡ tool_requestï¼ˆå·²åœ¨ä¸Šé¢å¤„ç†è¿‡ï¼‰
            if not step_json.get('tool_request'):
                try:
                    # ä½¿ç”¨ç»Ÿä¸€çš„ MCP å·¥å…·å…¥å£
                    logger.info(f"æ‰§è¡Œ MCP åŠ¨ä½œ: {action_type or 'unknown'}")
                    
                    # execute_step_json æ¥å— step å­—å…¸ï¼ŒåŒ…å« action/target/data/wait_after
                    step_params = {
                        "step": current_action,  # ç›´æ¥ä¼ å…¥ LLM ç”Ÿæˆçš„ current_action
                        "verify": True  # å¯ç”¨å‰åéªŒè¯
                    }
                    
                    # ç»Ÿä¸€è°ƒç”¨ execute_step_json
                    action_result = await server.execute_tool("execute_step_json", step_params)
                    
                    # è®°å½•æ‰§è¡Œç»“æœï¼ˆç»Ÿä¸€æ—¥å¿—åç§°ï¼‰
                    if action_result:
                        # è®°å½•å®Œæ•´ç»“æœåˆ° tools ç›®å½•ï¼ˆä½¿ç”¨ tool_ å‰ç¼€ï¼‰
                        dump_text(f"tool_mcp_execute_step_json_round_{round_idx}.json", 
                                json.dumps(action_result if isinstance(action_result, dict) else 
                                         (action_result.to_dict() if hasattr(action_result, 'to_dict') else str(action_result)), 
                                         ensure_ascii=False, indent=2))
                        
                        # è¿½åŠ  MCP è°ƒç”¨ç®€è¡¨åˆ° mcp_execution_logï¼ˆåŒ…å« exec_logï¼‰
                        try:
                            # Unwrap response to derive success/error/exec_log for logging
                            if hasattr(action_result, 'to_dict'):
                                ar = action_result.to_dict()  # MCPResponse
                                success_flag = bool(ar.get('success', False))
                                exec_log_data = ar.get('exec_log')
                                error_msg = ar.get('error')
                                result_summary = {
                                    "changed": ar.get('changed'),
                                    "invalid_action": ar.get('invalid_action'),
                                    "used": ar.get('used'),
                                    "message": ar.get('message')
                                }
                            elif isinstance(action_result, dict):
                                success_flag = bool(action_result.get('success', False))
                                exec_log_data = action_result.get('exec_log')
                                error_msg = action_result.get('error')
                                result_summary = {
                                    "changed": action_result.get('changed'),
                                    "invalid_action": action_result.get('invalid_action'),
                                    "used": action_result.get('used'),
                                    "message": action_result.get('message')
                                }
                            else:
                                success_flag = True
                                exec_log_data = None
                                error_msg = None
                                result_summary = None
                            
                            append_mcp_log(
                                tool="execute_step_json",
                                parameters=step_params,
                                success=success_flag,
                                result_dump_path=f"tools/tool_mcp_execute_step_json_round_{round_idx}.json",
                                phase="execution",
                                round_idx=round_idx,
                                exec_log=exec_log_data,  # åŒ…å«æ‰§è¡Œæ—¥å¿—
                                error=error_msg,
                                result_summary=result_summary  # åŒ…å«ç»“æœæ‘˜è¦
                            )
                        except Exception:
                            pass
                        
                        # æå–æ‰§è¡ŒçŠ¶æ€
                        if isinstance(action_result, dict):
                            success = bool(action_result.get('success', False))
                            changed = bool(action_result.get('changed', False))
                            invalid = bool(action_result.get('invalid_action', False))
                            exec_log_display = action_result.get('exec_log', [])
                            used_selector = action_result.get('used', {})
                            error_display = action_result.get('error')
                        else:
                            success = bool(getattr(action_result, 'success', False))
                            changed = bool(getattr(action_result, 'changed', False))
                            invalid = bool(getattr(action_result, 'invalid_action', False))
                            exec_log_display = getattr(action_result, 'exec_log', [])
                            used_selector = getattr(action_result, 'used', {})
                            error_display = getattr(action_result, 'error', None)
                        
                        # æ˜¾ç¤ºæ‰§è¡Œæ—¥å¿—ï¼ˆå®æ—¶æŸ¥çœ‹æ‰§è¡Œäº†ä»€ä¹ˆå‘½ä»¤ï¼‰
                        if exec_log_display:
                            logger.info(f"[EXEC LOG round {round_idx}]:")
                            for log_line in exec_log_display:
                                logger.info(f"  -> {log_line}")
                        
                        # æ˜¾ç¤ºä½¿ç”¨çš„é€‰æ‹©å™¨
                        if used_selector:
                            logger.info(f"[SELECTOR] {used_selector}")
                        
                        # æ˜¾ç¤ºéªŒè¯ç»“æœ
                        status_icon = "[OK]" if success else "[FAIL]"
                        logger.info(f"{status_icon} åŠ¨ä½œæ‰§è¡Œ{'æˆåŠŸ' if success else 'å¤±è´¥'}: {action_type} | å±å¹•{'å·²å˜åŒ–' if changed else 'æœªå˜åŒ–'} | {'æ— æ•ˆåŠ¨ä½œ' if invalid else 'æœ‰æ•ˆ'}")
                        
                        # æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
                        if error_display:
                            logger.warning(f"[ERROR] {error_display}")
                        
                except Exception as e:
                    # è®°å½•å¼‚å¸¸åˆ°æ—¥å¿—æ–‡ä»¶
                    logger.error(f"æ‰§è¡Œ MCP åŠ¨ä½œå¼‚å¸¸: {e}")
                    dump_text(f"error_execute_step_round_{round_idx}.txt", f"Exception: {e}\n\nStep JSON:\n{json.dumps(current_action, ensure_ascii=False, indent=2)}")
                    action_result = {"success": False, "error": str(e), "exception": True}
        
        # Round execution completed (è¯¦ç»†æ—¥å¿—å·²åœ¨ MCP å±‚è®°å½•)

    # ============================================================
    # COMPLETION é˜¶æ®µâ€”â€”æ—¥å¿—è®°å½•ä¸”ç»“æŸã€‚æ— å®é™…æ‰§è¡Œ
    # ============================================================
    # å½“å‰åº”ç”¨åœºæ™¯ï¼šä»…æ‰§è¡Œã€Œè§„åˆ’â†’æé—®ã€ä¸æ‰§è¡ŒåŠ¨ä½œ
    # å¦‚éœ€çœŸå®ä¸‹ç¬”æµ‹è¯•ï¼Œéœ€è¡¥ä¸Š:
    #  - perform_and_verify æˆ– perform_ui_action è°ƒç”¨
    #  - æ§åˆ¶æµç¨‹ï¼ˆæ ¹æ® current_action ç±»å‹æ‰§è¡Œï¼‰
    #  - ä¸€ä¸Šè½®çš„æ‰“å²”æç¤ºã€å¸®æ‰¶æç¤ºç­‰
    # ============================================================
    completion_prompt = (
        "# ä»…è¾“å‡ºä¸¥æ ¼JSONï¼Œæ•´åˆæ‰€æœ‰æ­¥éª¤ã€‚ä¸¥ç¦Markdownã€‚åªè¾“å‡ºä¸€ä¸ª JSON å¯¹è±¡ã€‚\n\n"
        f"æµ‹è¯•ç›®æ ‡: {args.requirement}\n\n"
        f"è§„åˆ’è¾“å‡º: {json.dumps(plan_json, ensure_ascii=False)}\n\n"
        f"æ­¥éª¤è¾“å‡º(æ‘˜è¦): {json.dumps(generated_steps, ensure_ascii=False)}\n\n"
        "è¯·è¾“å‡ºå®Œæ•´çš„ Only-Test JSON æµ‹è¯•ç”¨ä¾‹ï¼ˆæ¯æ­¥ä½¿ç”¨å…è®¸åŸå­åŠ¨ä½œï¼ŒåŒ…å« priority_selectors æˆ– bounds_pxï¼‰ã€‚\n"
    )
    dump_text("prompt_completion.txt", completion_prompt)
    comp_msgs = [{"role": "system", "content": "You are Only-Test LLM. Output strict JSON only."}, {"role": "user", "content": completion_prompt}]
    comp_resp = llm.chat_completion(comp_msgs, temperature=0.15, max_tokens=2000)
    dump_text("response_completion.txt", comp_resp.content or "")
    
    # æ‰“å° Completion Response åˆ°å‘½ä»¤è¡Œ
    logger.info("=" * 80)
    logger.info("COMPLETION RESPONSE:")
    if comp_resp.content:
        logger.info(comp_resp.content[:500] + ("...\n(å†…å®¹å·²æˆªæ–­ï¼Œå®Œæ•´å†…å®¹è§ response_completion.txt)" if len(comp_resp.content) > 500 else ""))
    else:
        logger.warning("Completion response is empty!")
    logger.info("=" * 80)

    # è§£ææœ€ç»ˆ JSONï¼Œå¹¶ä¿å­˜åˆ° --outdir
    final_case = {}
    try:
        final_case = _extract_json_robust(comp_resp.content or "", kind="final", required_keys=["test_steps","test_steps","hooks","target_app"]) or {}
    except Exception as e:
        dump_text("error_parse_completion.txt", f"{e}\n\nRAW:\n{comp_resp.content}")
        final_case = {}

    try:
        Path(args.outdir).mkdir(parents=True, exist_ok=True)
        json_name = f"generated_case_{session_id}.json"
        json_path = Path(args.outdir) / json_name
        if final_case:
            with open(json_path, 'w', encoding='utf-8') as f:
                f.write(json.dumps(final_case, ensure_ascii=False, indent=2))
            dump_text("artifact_final_case_path.txt", str(json_path))
        else:
            dump_text("artifact_final_case_path.txt", "(final case empty)")
    except Exception as e:
        dump_text("error_write_final_case.txt", str(e))

    # è‹¥ä¼ å…¥ --executeï¼šå°†æœ€ç»ˆ JSON è½¬ä¸º Pythonï¼Œå¹¶å°è¯•æ‰§è¡Œ
    if args.execute and final_case:
        try:
            import subprocess as _subp
            py_out_dir = Path('only_test/testcases/python')
            py_out_dir.mkdir(parents=True, exist_ok=True)
            py_out = py_out_dir / f"generated_{session_id}.py"
            # è°ƒç”¨ç”Ÿæˆå™¨
            _cmd = [sys.executable, 'only_test/tools/codegen/json_to_airtest.py', '--in', str(json_path), '--out', str(py_out)]
            _gen = _subp.run(_cmd, capture_output=True, text=True)
            dump_text("artifact_codegen_stdout.txt", _gen.stdout or "")
            dump_text("artifact_codegen_stderr.txt", _gen.stderr or "")
            if _gen.returncode != 0:
                logger.error("JSONâ†’Python è½¬æ¢å¤±è´¥")
            else:
                logger.info(f"å·²ç”Ÿæˆ Python ç”¨ä¾‹: {py_out}")
                # ç›´æ¥å°è¯•ç”¨ Python æ‰§è¡Œï¼ˆå¦‚éœ€ airtestï¼Œè¯·æ”¹ç”¨ `airtest run`ï¼‰
                _run = _subp.run([sys.executable, str(py_out)], capture_output=True, text=True)
                dump_text("artifact_run_generated_stdout.txt", _run.stdout or "")
                dump_text("artifact_run_generated_stderr.txt", _run.stderr or "")
                if _run.returncode != 0:
                    logger.warning("æ‰§è¡Œç”Ÿæˆçš„ Python ç”¨ä¾‹å¤±è´¥ï¼Œè¯·æ£€æŸ¥è¿æ¥/è®¾å¤‡/ä¾èµ–ï¼Œå¹¶è€ƒè™‘ä½¿ç”¨ airtest run")
                else:
                    logger.info("ç”Ÿæˆçš„ Python ç”¨ä¾‹æ‰§è¡Œå®Œæˆ")
        except Exception as e:
            dump_text("error_execute_generated.txt", str(e))

    # ============================================================
    # ä¼šè¯ç»“æŸ
    # ============================================================
    # æ—¥å¿—å·²åœ¨ execution_log.json ä¸­è¢«é€æ¡è®°å½•
    # ç»Ÿè®¡æ‘˜è¦ç”± unified_logger åœ¨ session_end è¾“å‡º
    logger.log_session_end({"note": "completed", "total_rounds": total_rounds, "generated_steps": len(generated_steps)})


if __name__ == "__main__":
    asyncio.run(main())
