丢失

一部分写的很好, 但我想补充一些内容, # ## page: search pgae, comment: click search button-either cancel search button, judge by box have conten or not . 这时用例文件里写的, 为什么需要它, 他是否 LLM 或
  人一眼就能看到要做什么呢, 如果有更加多的 meatadata 元信息写在 用例文件会使生成用例是一个极其简单的事情, 这就是我再 README 中定义各种 meata tag 等信息的意义. 请你考虑好如何将哪些信息结合到 用例文件中,
  这一一个需要深度思考的步骤 .C:\Download\git\uni\airtest\untitled.air\untitled.py  2.在 airtest 目录下创建一个目录结构, 它将是有 部分的 phone-use 代码(用来获取屏幕信息函等, 作为lib,)
  你需要做的是先定义好目录结构, 这个过程我会不断提醒你 我们打造一个更好的测试框架
  
我还希望你能思考: json 只是一个媒介, 用来统计用例, 之后要生成 Py 也是作为用例文件
  我的想法 为什么需要json,: 1.统计用例信息, 比如用例 meta 信息, 生成的截图信息等
  为什么需要 py 最终用例文件: 这个用例可能需要特殊的 func, py 更加灵活, 并且执行和生成报告等会使用 airtest+pytest+allure 结合的方式, 我们不可能打造全新的框架, 也没必要,
  因此我希望你好好想想他们的协作模式. 比如: json 转 py? 在执行用例前使用自写工具将我们定义的数据转为 py 代码后使用 airtest 执行. 这是一个参考, 你可以思考更多方式且不局限.
  
 完善后输出这个工作流程, 我需要通俗的给他人讲解这个项目是如何工作的, 工作流程:
  1.新的 apk 新的用例, 输入一个测试点, LLM 生成用例先假设为 json 文件
  2.试跑, 此时更新设备探测道德信息到 json. 比如设备清晰度 android 版本等, 之后开始run 用例
  3.run 用例的同时因为可能用的到 视觉识别, 因此图片等信息, 以及识别到的信息需要保留好, 以相对路径方式写在 json 子 item 中, 相对路径以 pkg name 和 phone name 方便区分, 具体区分规则有 README 有写
  4.
  
  
  
好的, 我们先着手完善 视觉与元素识别系统, 实际他是一个写完并验证的系统, 具体规则你可读取 /phone-use 目录, 它使用 fallback 模式进行不同界面的元素识别(只需要发送一个消息即可获取结果), 但对外部来说是无感的,
  我们这里也要支持动态切换策略(根据播放状态), 先检查 airtest/lib 下是否存在相关逻辑, 迁移到你希望的地方, 如果没有请从 phone-use 迁移过来, 这是独立不同的项目, 你也需要确保新功能狗简洁,
  同时你需思考下如何让别人明显知道你已经实现了这个功能且实现思路, 即使这不是你的原创, 记录好以便你的后 LLM 熟练掌握全局
  

请对每点再详细一些, 有一些步骤需要将配置信息回写到 yaml 或 json , 比如 分辨率信息, 场景是, 这方面我还不清楚是怎么实现的, 请你清晰的米描述流程, 注意越详细越好, 但要足够简洁, 避免冗余


----

1.检查当前的 omniparser 输出结果是什么 ,我看到其它地方直接使用 json 了, 你可以四送这张可用图过去实验世界结构, 验证当前结构是如设想"C:\Download\git\uni\zdep_OmniParser-v2-finetune\imgs\yc_vod_playing_fullscreen.png" 
2.很多注释或说明的功能是与当前项目无关的 比如 在抖音中搜索'美食视频'，如果搜索框有内容先清空, 请再遇到这些内容更改为测试 com.mobile.brasiltvmobile 的播放功能, 等, 避免引起后人对其疑惑
2.实验 MCP 功能, 尝试发送消息到 LLM, 确认其可用, 之后确认如何与我们创建的 MCP 进行交互, 你可以 mock 一段内容, 来进行尝试
3.最终那个目的是成功生成一个用例, 指定好 plan 后开始work, 注意, 不是你来生成,  而是模拟人一样使用另一个 LLM 来生成一个用例, 这才是这个项目的意义所在

---------



你好像误解了我的意思, 而且生成用例太复杂,  并且这应该也是你自己生成的把. uni\airtest\testcases\python\example_airtest_record.py 这就是一个标准用例的书写格式, 最终 json 会转为这种, json 中的 path 主要是为了记录, 而最终的 py 文件就是要如此简单用于执行用例测试,. 
流程控制,你应该也注意到 用例文件中是一步一步的执行的,而你也要要知道, 这个项目的作用就是让 LLM 根据已有用例 辅助生成新的功能点用例,问题只是其中的  com.mobile.brasiltvmobile:id/mImageFullScreen 或者 type anyway 这种控件名不好找, 因此需要 LLM 根据这一步, 比如当前要搜索, 那它需要找到搜索的 id/name 等元素信息记录到 json,  一步步的完成后比如全屏播放了, 此时结束任务. 我感觉你在做的内容一直是模糊不清的, 你有任务问题请问我, 因为我不知道你卡再哪一步. 这次我需要你先反思, 并把你的 plan 给我, 我修改并予以批准后再开始



  1. 当前问题定位
    - 控件名通过人工找到或录制太费时间, 让 外部 LLM 使用 MCP 监控屏幕或操纵屏幕 获取当前控件状态（如 com.mobile.brasiltvmobile:id/mImageFullScreen）之后记录操作到 json 

  1. 标准用例格式确认
    - example_airtest_record.py 是唯一的标准格式吗？ 是的, 他是最终用例的执行格式, 其内部也需要包含这种声明(## [page] vod_playing_detail, [action] click, [comment] 点击全屏按钮进入全屏播放模式). 但是 setup_hook 等可能会有额外的执行函数, 这是辅助项, 最终用例一定是 airtest 认可的这种格式, 它也是简介的用法. 你之前写的太复杂, 任何一个人都会否决你的用例文件
    - 有其他参考用例格式吗？无, 你可以先从用例文件 example_airtest_record.py 反推出一个 json 我会检查是否正确与符合要求, 以明确 用例标准格式 
  2. LLM的具体作用范围
    - 是否只负责元素识别和定位？
    - 还是还包括测试逻辑的规划？
    两者, 其需要自驱动的理解好我提供的测试功能点, 之后一步步的捕获屏幕 ID 并写一个 json 用例. 
  3. JSON结构定义
    - JSON中需要记录哪些具体信息？
    - path字段的具体作用是什么？ *path* 主要记录这个记录生成时 LLM 使用工具生成的结果, 你应该能想明白为什么需要记录对吗, 补充他. testcases\generated\tc_com_ss_android_ugc_aweme_20250905_184738.json  这里有一个之前自动生成的内容,但是它是不符合要求的, 你需要反推后生成一个符合要求的内容
  4. 目标应用和功能
    - 当前主要针对 com.mobile.brasiltvmobile 应用？ 是的, 我们目前的目标是打造框架后使用现有框架 only-test 写一个用例
    - 主要测试哪些功能点（搜索、播放、全屏等）？ 目前要测
  5. 现有工具集成
    - Omniparser的识别结果如何与LLM结合？ 这个问题你需要先了解 uiautomator2 是如何工作的, 他是直接对当前 android 界面元素做 captrue 之后计算详细位置对吗, 之后你可以尝试请求 Omniparser 获取其结果, 其有两个重要信息, content 和 bbox, 你应该知道这些是什么, 而剩下的内容可以解决你的疑惑, 模式:  dumo_ui + omniparser 模式
作用: dump_ui 使用 uiautomator2 , 即使如此, 在使用 APK 播放状态依旧无法获取界面控件布局, 只有当暂停播放视频时才可以, 此时需要 omniparser 进行基于 AI 的 UI 视觉识别, 其能感知当前屏幕的 app 空间位置, 之后, 返回给外部 LLM 的是与 Dump_ui 一样的元素位置等详细信息, 使用者并不知道两者的区别 只需要关注空间是否成功点击, 达成操控目的. 
既然  uiautomator2 有问题为什么不全部使用 omniparser: omniparser 识别速度没有 uiautomator2 快, 准确率 90%, 即使如此也有错误率, 其使用 GPU 资源消耗

下一步, 对于外部 LLM 而言其不需要知道内部逻辑, 因此code 需要自动根据当前播放状态来确认使用哪个进行屏幕识别, 比如可以通过当前媒体播放状态, 音频使用状态, player 使用状态来确认播放状态, 之后使用正确的识别模式, 精准切换

    - MCP工具在这个流程中的具体作用？ MCP 是为了让外部 LLM 主动capture 当前屏幕, 并 filter 其需要的内容, 获取 id 等控件信息后其才能正常确认控件 ID 等信息, 才能写用例, 对吗


我需要你做这些事情
1.反推 json 文件
2.补充我们的问题记录后记录到 QA.md 之后生成更多问题, 以及你的猜想, 我来检查, 并可能会纠正你的猜想, 我希望能通过这份文档纠正你的想法



-----



请你再生成当前 airtest 目录下的结构信息,补充每一个文件的作用到其中,我会在你补充后统一纠正
(感悟: 当 vibecoding 到迷茫怎么办, 离目标很远, 但是不知道下一步要做什么 
这就是再教导一个 mem 有限的孩童做事情, 你需要知道它的记忆有限, 因此要尽可能的简单又清晰的做好一切描述, 才能确保走在正确的路上, 否则只是原地
这个问答过程也是让你自己更了解这个项目内容的过程)

