丢失

一部分写的很好, 但我想补充一些内容, # ## page: search pgae, comment: click search button-either cancel search button, judge by box have conten or not . 这时用例文件里写的, 为什么需要它, 他是否 LLM 或
  人一眼就能看到要做什么呢, 如果有更加多的 meatadata 元信息写在 用例文件会使生成用例是一个极其简单的事情, 这就是我再 README 中定义各种 meata tag 等信息的意义. 请你考虑好如何将哪些信息结合到 用例文件中,
  这一一个需要深度思考的步骤 .C:\Download\git\uni\airtest\untitled.air\untitled.py  2.在 airtest 目录下创建一个目录结构, 它将是有 部分的 phone-use 代码(用来获取屏幕信息函等, 作为lib,)
  你需要做的是先定义好目录结构, 这个过程我会不断提醒你 我们打造一个更好的测试框架
  
我还希望你能思考: json 只是一个媒介, 用来统计用例, 之后要生成 Py 也是作为用例文件
  我的想法 为什么需要json,: 1.统计用例信息, 比如用例 meta 信息, 生成的截图信息等
  为什么需要 py 最终用例文件: 这个用例可能需要特殊的 func, py 更加灵活, 并且执行和生成报告等会使用 airtest+pytest+allure 结合的方式, 我们不可能打造全新的框架, 也没必要,
  因此我希望你好好想想他们的协作模式. 比如: json 转 py? 在执行用例前使用自写工具将我们定义的数据转为 py 代码后使用 airtest 执行. 这是一个参考, 你可以思考更多方式且不局限.
  
 完善后输出这个工作流程, 我需要通俗的给他人讲解这个项目是如何工作的, 工作流程:
  1.新的 apk 新的用例, 输入一个测试点, LLM 生成用例先假设为 json 文件
  2.试跑, 此时更新设备探测道德信息到 json. 比如设备清晰度 android 版本等, 之后开始run 用例
  3.run 用例的同时因为可能用的到 视觉识别, 因此图片等信息, 以及识别到的信息需要保留好, 以相对路径方式写在 json 子 item 中, 相对路径以 pkg name 和 phone name 方便区分, 具体区分规则有 README 有写
  4.
  
  
  
好的, 我们先着手完善 视觉与元素识别系统, 实际他是一个写完并验证的系统, 具体规则你可读取 /phone-use 目录, 它使用 fallback 模式进行不同界面的元素识别(只需要发送一个消息即可获取结果), 但对外部来说是无感的,
  我们这里也要支持动态切换策略(根据播放状态), 先检查 airtest/lib 下是否存在相关逻辑, 迁移到你希望的地方, 如果没有请从 phone-use 迁移过来, 这是独立不同的项目, 你也需要确保新功能狗简洁,
  同时你需思考下如何让别人明显知道你已经实现了这个功能且实现思路, 即使这不是你的原创, 记录好以便你的后 LLM 熟练掌握全局
  

请对每点再详细一些, 有一些步骤需要将配置信息回写到 yaml 或 json , 比如 分辨率信息, 场景是, 这方面我还不清楚是怎么实现的, 请你清晰的米描述流程, 注意越详细越好, 但要足够简洁, 避免冗余


----

1.检查当前的 omniparser 输出结果是什么 ,我看到其它地方直接使用 json 了, 你可以四送这张可用图过去实验世界结构, 验证当前结构是如设想"C:\Download\git\uni\zdep_OmniParser-v2-finetune\imgs\yc_vod_playing_fullscreen.png"
